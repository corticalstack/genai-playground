{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5a69d5d13a548f5a435d18c947ee533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='abideen/Heimer-dpo-TinyLlama-1.1B', description='Model ID')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "first_name_input = widgets.Text(\n",
    "    value='abideen/Heimer-dpo-TinyLlama-1.1B',\n",
    "    description='Model ID',\n",
    "    disabled=False\n",
    ")\n",
    "display(first_name_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67fe2baeace74d5ca963e57a9981b1d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=1, description='Number:', max=10)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea15987685c4261ab9dbed8b2d7a842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Function to square the input number\n",
    "def square_number(x):\n",
    "    print(f\"The square of {x} is {x*x}\")\n",
    "\n",
    "# Create an integer slider widget\n",
    "int_slider = widgets.IntSlider(\n",
    "    value=1,  # initial value\n",
    "    min=0,    # minimum value\n",
    "    max=10,   # maximum value\n",
    "    step=1,   # step size\n",
    "    description='Number:'\n",
    ")\n",
    "\n",
    "# Display the slider\n",
    "display(int_slider)\n",
    "\n",
    "# Add an output widget to display the results\n",
    "output = widgets.Output()\n",
    "display(output)\n",
    "\n",
    "# Function to update the output when the slider value changes\n",
    "def on_value_change(change):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        square_number(change['new'])\n",
    "\n",
    "# Link the function to the value change event\n",
    "int_slider.observe(on_value_change, names='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title # ü§ñ AutoQuantize\n",
    "\n",
    "# @markdown üîÆ Created by [@zainulabideen](https://huggingface.co/abideen).\n",
    "\n",
    "# @markdown Please add HF token to the secrets tab in Google Colab before.\n",
    "\n",
    "# @markdown Quantization formats supported: `GGUF`, `AWQ`, `EXL2`, `GPTQ`\n",
    "\n",
    "# @markdown ---\n",
    "\n",
    "\n",
    "# @markdown ### ü§ó Hugging Face Hub\n",
    "\n",
    "MODEL_ID = \"abideen/Heimer-dpo-TinyLlama-1.1B\" # @param {type:\"string\"}\n",
    "MODEL_NAME = MODEL_ID.split('/')[-1]\n",
    "\n",
    "# Download model\n",
    "!git lfs install\n",
    "!git clone https://huggingface.co/{MODEL_ID}\n",
    "\n",
    "username = \"abideen\" # @param {type:\"string\"}\n",
    "token = \"\" # @param {type:\"string\"}\n",
    "!pip install -q huggingface_hub\n",
    "from huggingface_hub import create_repo, HfApi\n",
    "from google.colab import userdata, runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title # üõ∏ GGUF\n",
    "# @markdown ### ‚ú® Quantization parameters\n",
    "\n",
    "QUANTIZATION_FORMAT = \"q4_k_m\" # @param {type:\"string\"}\n",
    "QUANTIZATION_METHODS = QUANTIZATION_FORMAT.replace(\" \", \"\").split(\",\")\n",
    "# Install llama.cpp\n",
    "!git clone https://github.com/ggerganov/llama.cpp\n",
    "!cd llama.cpp && git pull && make clean && LLAMA_CUBLAS=1 make\n",
    "!pip install -r llama.cpp/requirements.txt\n",
    "\n",
    "# Convert to fp16\n",
    "fp16 = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.fp16.bin\"\n",
    "!python llama.cpp/convert.py {MODEL_NAME} --outtype f16 --outfile {fp16}\n",
    "\n",
    "# Quantize the model for each method in the QUANTIZATION_METHODS list\n",
    "for method in QUANTIZATION_METHODS:\n",
    "    qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf\"\n",
    "    !./llama.cpp/quantize {fp16} {qtype} {method}\n",
    "\n",
    "# Defined in the secrets tab in Google Colab\n",
    "hf_token = userdata.get(token)\n",
    "api = HfApi()\n",
    "\n",
    "# Create empty repo\n",
    "create_repo(\n",
    "    repo_id = f\"{username}/{MODEL_NAME}-GGUF\",\n",
    "    repo_type=\"model\",\n",
    "    exist_ok=True,\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "# Upload gguf files\n",
    "api.upload_folder(\n",
    "    folder_path=MODEL_NAME,\n",
    "    repo_id=f\"{username}/{MODEL_NAME}-GGUF\",\n",
    "    allow_patterns=[\"*.gguf\",\"$.md\"],\n",
    "    token=hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title # üèõÔ∏è AWQ\n",
    "# @markdown ### ‚ú® Quantization parameters\n",
    "\n",
    "Q_GROUP_SIZE = 128 # @param {type:\"integer\"}\n",
    "ZERO_POINT = True # @param {text:\"boolean\"}\n",
    "W_BIT = 4 # @param {type:\"integer\"}\n",
    "VERSION = \"GEMM\" # @param {type:\"string\"}\n",
    "SAFETENSORS = True # @param {text:\"boolean\"}\n",
    "\n",
    "# Install AutoAWQ\n",
    "!git clone https://github.com/casper-hansen/AutoAWQ\n",
    "%cd AutoAWQ\n",
    "!pip install -e .\n",
    "!pip install git+https://github.com/huggingface/transformers\n",
    "!pip install zstandard\n",
    "\n",
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "quant_path = MODEL_NAME + \"-awq\"\n",
    "quant_config = { \"zero_point\": ZERO_POINT, \"q_group_size\": Q_GROUP_SIZE, \"w_bit\": W_BIT, \"version\": VERSION }\n",
    "\n",
    "# Load model\n",
    "PATH = \"/content/\" + MODEL_NAME\n",
    "model = AutoAWQForCausalLM.from_pretrained(PATH, safetensors=SAFETENSORS)\n",
    "tokenizer = AutoTokenizer.from_pretrained(PATH, trust_remote_code=True)\n",
    "\n",
    "# Quantize\n",
    "model.quantize(tokenizer, quant_config=quant_config)\n",
    "\n",
    "# Save quantized model\n",
    "model.save_quantized(quant_path)\n",
    "tokenizer.save_pretrained(quant_path)\n",
    "\n",
    "# Defined in the secrets tab in Google Colab\n",
    "hf_token = userdata.get(token)\n",
    "api = HfApi()\n",
    "\n",
    "# Create empty repo\n",
    "create_repo(\n",
    "    repo_id = f\"{username}/{MODEL_NAME}-AWQ\",\n",
    "    repo_type=\"model\",\n",
    "    exist_ok=True,\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "# Upload awq files\n",
    "api.upload_folder(\n",
    "    folder_path=quant_path,\n",
    "    repo_id=f\"{username}/{MODEL_NAME}-AWQ\",\n",
    "    token=hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title # üî¨ EXL2\n",
    "# @markdown ### ‚ú® Quantization parameters\n",
    "\n",
    "BPW = 5.0 # @param {type:\"number\"}\n",
    "\n",
    "# Install ExLLamaV2\n",
    "!git clone https://github.com/turboderp/exllamav2\n",
    "!pip install -e exllamav2\n",
    "\n",
    "!mv {MODEL_NAME} base_model\n",
    "!rm base_mode/*.bin\n",
    "\n",
    "# Download dataset\n",
    "!wget https://huggingface.co/datasets/wikitext/resolve/9a9e482b5987f9d25b3a9b2883fc6cc9fd8071b3/wikitext-103-v1/wikitext-test.parquet\n",
    "\n",
    "# Quantize model\n",
    "!mkdir quant\n",
    "!python exllamav2/convert.py \\\n",
    "    -i base_model \\\n",
    "    -o quant \\\n",
    "    -c wikitext-test.parquet \\\n",
    "    -b {BPW}\n",
    "\n",
    "# Copy files\n",
    "!rm -rf quant/out_tensor\n",
    "!rsync -av --exclude='*.safetensors' --exclude='.*' ./base_model/ ./quant/\n",
    "\n",
    "# Defined in the secrets tab in Google Colab\n",
    "hf_token = userdata.get(token)\n",
    "api = HfApi()\n",
    "\n",
    "# Create empty repo\n",
    "create_repo(\n",
    "    repo_id = f\"{username}/{MODEL_NAME}-{BPW:.1f}bpw-exl2\",\n",
    "    repo_type=\"model\",\n",
    "    exist_ok=True,\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "# Upload exl2 files\n",
    "api.upload_folder(\n",
    "    folder_path=quant,\n",
    "    repo_id=f\"{username}/{MODEL_NAME}-{BPW:.1f}bpw-exl2\",\n",
    "    token=hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title # üìù GPTQ\n",
    "# @markdown ### ‚ú® Quantization parameters\n",
    "\n",
    "BITS = 4 # @param {type:\"integer\"}\n",
    "GROUP_SIZE = 128 # @param {type:\"integer\"}\n",
    "DAMP_PERCENT = 0.01 # @param {type:\"number\"}\n",
    "\n",
    "!BUILD_CUDA_EXT=0 pip install -q auto-gptq transformers\n",
    "import random\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "out_dir = MODEL_ID + \"-GPTQ\"\n",
    "\n",
    "# Load quantize config, model and tokenizer\n",
    "quantize_config = BaseQuantizeConfig(\n",
    "    bits=BITS,\n",
    "    group_size=GROUP_SIZE,\n",
    "    damp_percent=DAMP_PERCENT,\n",
    "    desc_act=False,\n",
    ")\n",
    "PATH = \"/content/\" + MODEL_NAME\n",
    "model = AutoGPTQForCausalLM.from_pretrained(PATH, quantize_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(PATH)\n",
    "\n",
    "# Load data and tokenize examples\n",
    "n_samples = 1024\n",
    "data = load_dataset(\"allenai/c4\", data_files=\"en/c4-train.00001-of-01024.json.gz\", split=f\"train[:{n_samples*5}]\")\n",
    "tokenized_data = tokenizer(\"\\n\\n\".join(data['text']), return_tensors='pt')\n",
    "\n",
    "# Format tokenized examples\n",
    "examples_ids = []\n",
    "for _ in range(n_samples):\n",
    "    i = random.randint(0, tokenized_data.input_ids.shape[1] - tokenizer.model_max_length - 1)\n",
    "    j = i + tokenizer.model_max_length\n",
    "    input_ids = tokenized_data.input_ids[:, i:j]\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    examples_ids.append({'input_ids': input_ids, 'attention_mask': attention_mask})\n",
    "\n",
    "# Quantize with GPTQ\n",
    "model.quantize(\n",
    "    examples_ids,\n",
    "    batch_size=1,\n",
    "    use_triton=True,\n",
    ")\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_quantized(out_dir, use_safetensors=True)\n",
    "tokenizer.save_pretrained(out_dir)\n",
    "\n",
    "# Defined in the secrets tab in Google Colab\n",
    "hf_token = userdata.get(token)\n",
    "api = HfApi()\n",
    "\n",
    "# Create empty repo\n",
    "create_repo(\n",
    "    repo_id = f\"{username}/{MODEL_NAME}-GPTQ\",\n",
    "    repo_type=\"model\",\n",
    "    exist_ok=True,\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "# Upload gptq files\n",
    "api.upload_folder(\n",
    "    folder_path=out_dir,\n",
    "    repo_id=f\"{username}/{MODEL_NAME}-GPTQ\",\n",
    "    token=hf_token\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genaiplayground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
