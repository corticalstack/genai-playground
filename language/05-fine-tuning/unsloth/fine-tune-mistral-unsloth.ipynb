{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload custom .jsonl collection of city information extraction instructions and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'What are some of the best museums to visit in Paris?', 'response': 'Paris is a city rich in museums, catering to a wide range of interests. Here are some you might want to consider:\\n\\n1. The Louvre: The world\\'s largest art museum and a historic monument in Paris, home to thousands of works of art, including the Mona Lisa.\\n\\n2. Musée d\\'Orsay: Located in a former railway station, this museum houses a large collection of Impressionist and Post-Impressionist masterpieces.\\n\\n3. Centre Pompidou: Known for its high-tech architecture, this museum houses the National Museum of Modern Art.\\n\\n4. Musée de l\\'Orangerie: Located in the Tuileries Gardens, it\\'s famous for Monet\\'s \"Water Lilies\" series.\\n\\n5. Musée Rodin: Dedicated to the works of the French sculptor Auguste Rodin.\\n\\n6. Musée du Quai Branly: Features indigenous art, cultures and civilizations from Africa, Asia, Oceania, and the Americas.\\n\\n7. Musée Picasso: Home to an impressive collection of more than 5,000 works by Pablo Picasso.\\n\\nRemember, many museums offer reduced or free admission on certain days or times, and you can also consider getting the Paris Museum Pass for unlimited access to over 60 museums and monuments.'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "file_path = '../cities.jsonl'\n",
    "\n",
    "data = []\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        json_line = json.loads(line.strip())\n",
    "        data.append(json_line)\n",
    "\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform city list of dictionaries into a list of dicts, which is suitable for Huggingface Dataset.from_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data = {}\n",
    "for key in data[0].keys():  # initialize keys\n",
    "    transformed_data[key] = []\n",
    "\n",
    "for item in data:\n",
    "    for key in item:\n",
    "        transformed_data[key].append(item[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Huggingface dataset from list of dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': ['What are some of the best museums to visit in Paris?', 'What is the local cuisine like in Paris?', 'What are some popular outdoor activities in Paris?', 'What is the currency used in Paris and where can I exchange my money?', 'What are some of the most famous landmarks in Paris?', 'What is the best time of year to visit Paris?', 'What is the weather like in Paris throughout the year?', 'What are some popular outdoor activities in Paris?', 'What are some popular markets in Paris that I should visit?'], 'response': ['Paris is a city rich in museums, catering to a wide range of interests. Here are some you might want to consider:\\n\\n1. The Louvre: The world\\'s largest art museum and a historic monument in Paris, home to thousands of works of art, including the Mona Lisa.\\n\\n2. Musée d\\'Orsay: Located in a former railway station, this museum houses a large collection of Impressionist and Post-Impressionist masterpieces.\\n\\n3. Centre Pompidou: Known for its high-tech architecture, this museum houses the National Museum of Modern Art.\\n\\n4. Musée de l\\'Orangerie: Located in the Tuileries Gardens, it\\'s famous for Monet\\'s \"Water Lilies\" series.\\n\\n5. Musée Rodin: Dedicated to the works of the French sculptor Auguste Rodin.\\n\\n6. Musée du Quai Branly: Features indigenous art, cultures and civilizations from Africa, Asia, Oceania, and the Americas.\\n\\n7. Musée Picasso: Home to an impressive collection of more than 5,000 works by Pablo Picasso.\\n\\nRemember, many museums offer reduced or free admission on certain days or times, and you can also consider getting the Paris Museum Pass for unlimited access to over 60 museums and monuments.', \"Parisian cuisine is renowned worldwide for its quality and diversity. Here are some dishes and food items you should try:\\n\\n1. Baguette: A long, thin loaf of French bread that is commonly made from basic lean dough.\\n\\n2. Croissant: A buttery, flaky, viennoiserie pastry of Austrian origin, named for its historical crescent shape.\\n\\n3. Escargot: Snails cooked typically in a garlic-butter, chicken soup or wine sauce.\\n\\n4. Coq au Vin: A traditional French dish of chicken slow-cooked in a wine-based sauce with mushrooms and possibly garlic.\\n\\n5. Ratatouille: A stewed vegetable dish consisting of peppers, eggplant, zucchini, and tomatoes—typical components of Provençal cuisine.\\n\\n6. Crêpes: Thin pancakes that can be filled with a variety of fillings, both sweet and savory.\\n\\n7. Cheese: France is famous for its variety of cheeses. Brie, Camembert, Roquefort are just a few to name.\\n\\n8. Pastries: Paris is heaven for pastry lovers. Don't miss trying a Paris-Brest, éclair, or tarte tatin.\\n\\n9. Wine: France is one of the largest wine producers in the world. Whether you prefer red, white, or rosé, there's something for every wine lover.\\n\\nRemember, one of the joys of visiting Paris is discovering the local bistros, cafés, and markets, and trying the wonderful array of food on offer.\", \"Paris offers a variety of outdoor activities for all ages and interests. Here are a few suggestions:\\n\\n1. Boating on the Seine: Rent a boat and enjoy a leisurely trip down the Seine River.\\n\\n2. Picnicking in a Park: Paris has many beautiful parks, such as the Tuileries Garden or the Luxembourg Garden, where you can have a picnic.\\n\\n3. Cycling: Rent a bike from the city's bike-sharing program, Vélib', and explore the city's many bike paths.\\n\\n4. Walking Tours: Join a guided walking tour to learn about the city's history and architecture.\\n\\n5. Visit the Zoos: The Ménagerie, the zoo in the Jardin des Plantes, and the Paris Zoological Park in the Bois de Vincennes, are great for families.\\n\\n6. Outdoor Markets: Visit one of the city's many outdoor markets to shop for food, antiques, art, and more.\\n\\n7. Hot Air Balloon Ride: Take a hot air balloon ride in the Parc André Citroën for a unique view of the city.\\n\\nRemember to check the weather and dress appropriately for your outdoor activities.\", 'The currency used in Paris, and all of France, is the Euro (€). You can exchange your money at various places:\\n\\n1. Banks: Most banks in Paris offer currency exchange services. However, they may charge a service fee and their hours of operation may not be as flexible as other options.\\n\\n2. Currency Exchange Bureaus: These are found throughout the city, especially in tourist areas and near major train stations. They usually offer competitive exchange rates, but it\\'s a good idea to compare rates at a few different places.\\n\\n3. ATMs: You can withdraw Euros directly from ATMs (called \"distributeurs automatiques de billets\" in French) found all over the city. This can be one of the most convenient and cost-effective ways to get cash, depending on your bank\\'s foreign transaction fees.\\n\\n4. Hotels: Some hotels offer currency exchange services, but their rates are usually not as good as banks or currency exchange bureaus.\\n\\nRemember to notify your bank of your travel plans to avoid any issues with your card while abroad. Also, it\\'s a good idea to have some cash on hand, but many places in Paris also accept credit cards.', \"Paris is known for its iconic landmarks. Here are some of the most famous ones:\\n\\n1. Eiffel Tower: This iconic tower is a must-visit. You can either admire it from the ground or climb up for a stunning view of the city.\\n\\n2. Notre-Dame Cathedral: Although it was damaged by a fire in 2019, this Gothic cathedral is still a sight to behold.\\n\\n3. Louvre Museum: Home to thousands of works of art, including the Mona Lisa, the Louvre is the world's largest art museum.\\n\\n4. Montmartre and the Sacré-Cœur: This historic and artistic neighborhood is known for its bohemian past and the white-domed Basilica of the Sacré-Cœur on its summit.\\n\\n5. Champs-Élysées and the Arc de Triomphe: The Champs-Élysées is a famous avenue known for its luxury shops, cafes, and theaters, ending at the Arc de Triomphe.\\n\\n6. Palace of Versailles: Just outside Paris, this opulent palace and garden is a UNESCO World Heritage site.\\n\\n7. Seine River and its bridges: The Seine River is lined with some of Paris's most famous landmarks, and its bridges, like the Pont Alexandre III and Pont Neuf, are attractions in their own right.\\n\\n8. The Panthéon: A neoclassical building in the Latin Quarter, it houses the remains of notable French citizens.\\n\\nRemember, it's best to check the opening times and book tickets in advance to avoid long lines.\", \"The best time to visit Paris really depends on what you're looking for. \\n\\nSpring (April to June) and Fall (September to November) are generally the most pleasant times to visit, with milder weather and fewer crowds. The city is particularly beautiful in the spring when the flowers are in bloom, and in the fall when the leaves are changing colors.\\n\\nSummer (July and August) can be quite hot and crowded, as this is peak tourist season. However, many Parisians leave the city in August for vacation, which can make the city feel less crowded. There are also many outdoor festivals and events during the summer.\\n\\nWinter (December to February) can be cold and rainy, but you'll find fewer tourists and lower prices. Plus, you'll get to see the city's beautiful Christmas decorations and markets.\\n\\nUltimately, there's no bad time to visit Paris. Each season offers its own unique charm and experiences.\", \"Paris has a temperate climate with distinct seasons:\\n\\n1. Spring (March to May): The weather starts to warm up with average temperatures ranging from 11°C to 20°C. It's a great time to see the city's parks and gardens in bloom.\\n\\n2. Summer (June to August): It can get quite hot with temperatures often reaching above 25°C. It's the busiest tourist season, and there are many outdoor festivals and events.\\n\\n3. Autumn (September to November): Temperatures cool down, ranging from 11°C to 20°C. The city is beautiful with fall foliage, and it's less crowded with tourists.\\n\\n4. Winter (December to February): It can get quite cold with temperatures often dropping to around 3°C. Snow is rare but possible. It's a good time to visit museums and enjoy indoor activities.\\n\\nRemember, the weather can be unpredictable, so it's always a good idea to check the forecast before your trip and pack accordingly.\", \"Paris offers a variety of outdoor activities for all ages and interests. Here are a few suggestions:\\n\\n1. Boating on the Seine: You can rent a boat or take a river cruise to see some of the city's most famous landmarks.\\n\\n2. Cycling: Paris is a bike-friendly city with many bike rental services and dedicated cycling paths.\\n\\n3. Picnicking: Parisians love to picnic in the city's many parks and gardens. Some popular spots include the Champ de Mars, Jardin du Luxembourg, and Buttes-Chaumont.\\n\\n4. Walking Tours: Explore the city on foot with a guided tour, or at your own pace. Don't miss the historic neighborhoods of Le Marais and Montmartre.\\n\\n5. Outdoor Markets: Visit one of the city's many outdoor markets to shop for fresh produce, cheese, pastries, and more.\\n\\n6. Street Performances: Enjoy free street performances in areas like the Pompidou Centre or along the Seine.\\n\\n7. Visit the Zoos: The Ménagerie, the zoo of the Jardin des Plantes, and the Paris Zoological Park are great for families.\\n\\nRemember, the best way to experience Paris is often to simply wander and see where the city takes you.\", \"Paris is known for its vibrant markets, each offering a unique shopping experience. Here are a few you should consider visiting:\\n\\n1. Marché des Enfants Rouges: The oldest covered market in Paris, known for its food stalls offering a variety of international cuisines.\\n\\n2. Marché d'Aligre: A daily market known for its fresh produce, cheese, and flowers. It also has a flea market.\\n\\n3. Marché aux Fleurs et aux Oiseaux: A flower and bird market located on Île de la Cité.\\n\\n4. Marché de la Création: An art market where local artists sell their work.\\n\\n5. Marché aux Puces de Saint-Ouen: One of the largest flea markets in the world, perfect for antiques and vintage items.\\n\\n6. Marché Biologique Raspail: A large organic market that takes place on Sundays.\\n\\n7. Marché Bastille: One of the biggest markets in Paris, offering a wide range of goods, from fresh produce to clothing.\\n\\nRemember, markets can be busy places, so keep an eye on your belongings and be prepared to haggle for the best prices.\"]}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "dataset = Dataset.from_dict(transformed_data)\n",
    "print(transformed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the base pre-trained model and tokenizer\n",
    "- Unsloth has it's own from_pretrained method.\n",
    "- \"load_in_4bit\" indicates that the model will be quantized with bitsandbytes NormalFloat4 data type. This is the standard data type for QLoRA fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "tokenizers>=0.11.1,!=0.11.3,<0.14 is required for a normal functioning of this module, but found tokenizers==0.15.0.\nTry: pip install transformers -U or pip install -e '.[dev]' if you're working with git main",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munsloth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m MAX_SEQ_LENGTH \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2048\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/genaiplayground/lib/python3.9/site-packages/unsloth/__init__.py:84\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA is not linked properly.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[1;32m     80\u001b[0m                           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe tried running `ldconfig /usr/lib64-nvidia` ourselves, but it didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt work.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[1;32m     81\u001b[0m                           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to run in your terminal `ldconfig /usr/lib64-nvidia` yourself, then import Unsloth.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msave\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/genaiplayground/lib/python3.9/site-packages/unsloth/models/__init__.py:15\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllama\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastLlamaModel\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmistral\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastMistralModel\n",
      "File \u001b[0;32m~/.conda/envs/genaiplayground/lib/python3.9/site-packages/unsloth/models/loader.py:15\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllama\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastLlamaModel, logger\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmistral\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastMistralModel\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoConfig\n",
      "File \u001b[0;32m~/.conda/envs/genaiplayground/lib/python3.9/site-packages/unsloth/models/llama.py:18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional, Tuple, List, Union\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m scaled_dot_product_attention\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllama\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_llama\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     19\u001b[0m     logger,\n\u001b[1;32m     20\u001b[0m     BaseModelOutputWithPast,\n\u001b[1;32m     21\u001b[0m     CausalLMOutputWithPast,\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkernels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/genaiplayground/lib/python3.9/site-packages/transformers/__init__.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[1;32m     29\u001b[0m     _LazyModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m     logging,\n\u001b[1;32m     43\u001b[0m )\n\u001b[1;32m     46\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/genaiplayground/lib/python3.9/site-packages/transformers/dependency_versions_check.py:57\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[1;32m     55\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# not required, check version only if installed\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     \u001b[43mrequire_version_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpkg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdeps\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, check dependency_versions_table.py\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/genaiplayground/lib/python3.9/site-packages/transformers/utils/versions.py:117\u001b[0m, in \u001b[0;36mrequire_version_core\u001b[0;34m(requirement)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"require_version wrapper which emits a core-specific hint on failure\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m hint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTry: pip install transformers -U or pip install -e \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.[dev]\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m if you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre working with git main\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequire_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequirement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhint\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/genaiplayground/lib/python3.9/site-packages/transformers/utils/versions.py:111\u001b[0m, in \u001b[0;36mrequire_version\u001b[0;34m(requirement, hint)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m want_ver \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m op, want_ver \u001b[38;5;129;01min\u001b[39;00m wanted\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 111\u001b[0m         \u001b[43m_compare_versions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgot_ver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwant_ver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequirement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpkg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhint\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/genaiplayground/lib/python3.9/site-packages/transformers/utils/versions.py:44\u001b[0m, in \u001b[0;36m_compare_versions\u001b[0;34m(op, got_ver, want_ver, requirement, pkg, hint)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to compare versions for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequirement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: need=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwant_ver\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m found=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgot_ver\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is unusual. Consider\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m reinstalling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     42\u001b[0m     )\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops[op](version\u001b[38;5;241m.\u001b[39mparse(got_ver), version\u001b[38;5;241m.\u001b[39mparse(want_ver)):\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequirement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is required for a normal functioning of this module, but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m==\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgot_ver\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     46\u001b[0m     )\n",
      "\u001b[0;31mImportError\u001b[0m: tokenizers>=0.11.1,!=0.11.3,<0.14 is required for a normal functioning of this module, but found tokenizers==0.15.0.\nTry: pip install transformers -U or pip install -e '.[dev]' if you're working with git main"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "LOAD_IN_4BIT = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/mistral-7b-bnb-4bit\", # \"unsloth/mistral-7b\" for 16bit loading\n",
    "    max_seq_length = MAX_SEQ_LENGTH,\n",
    "    dtype = None, # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "    load_in_4bit = LOAD_IN_4BIT # Use 4bit quantization to reduce memory usage. Can be False.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform our prompt/response dataset into Alpaca prompt template format \n",
    "Note by default, the Mistral tokenizer only adds <s> (BOS token) to the prompt but not </s> (EOS token), so (EOS) token added at end of prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token  # Will add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "def format_prompts(samples):\n",
    "    instructions = samples[\"prompt\"]\n",
    "    responses = samples[\"response\"]\n",
    "    texts = []\n",
    "    for prompt, response in zip(instructions, responses):\n",
    "        text = alpaca_prompt.format(prompt, \"\", response) + EOS_TOKEN # Input is empty as no completion history\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "dataset_in_prompt_format = dataset.map(format_prompts, batched = True,)\n",
    "print(dataset_in_prompt_format[0]['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the dataset into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = dataset_in_prompt_format.train_test_split(test_size=0.1)  # 10% for testing\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do model patching and add fast LoRA weights\n",
    "r and lora_aplha are the most important parameters in LoRA configuration.\n",
    "\n",
    "** r is the rank of the LoRA matrices:\n",
    "- A higher r-value means more trainable parameters, allowing for more expressivity. But, on the negative side, there is a compute tradeoff, and may also lead to overfitting.\n",
    "- A lower r-value means less trainable parameters, it can reduce overfitting at the cost of expressiveness.\n",
    "\n",
    "\n",
    "** lora_aplha is a scaling factor for LoRA weights:\n",
    "- Higher alpha will put more emphasis on LoRA weights.\n",
    "- Lower alpha will put reduced emphasis on LoRA weights, hence model will be more dependent on its original weights.\n",
    "\n",
    "\n",
    "** Important tips:\n",
    "- Golden rule: lora_aplha = 2*r, i.e., if r=128 and lora_aplha should be 256\n",
    "- Both r and lora_aplha should be in 2**x value, a good range for selection will be [8, 16, 32, 64, 128, 256, 512]\n",
    "- If your fine-tuning data is very different from the pre-training data of your model, I recommend selecting r and lora_aplha from the higher values from the above range and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = True,\n",
    "    random_state = 3407,\n",
    "    max_seq_length = MAX_SEQ_LENGTH,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define training arguments\n",
    "Train for MAX_STEPS with a total batch size of 24 (per_device_train_batch_size*gradient_accumulation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_STEPS=100\n",
    "from transformers import TrainingArguments\n",
    "training_arguments = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        do_eval=True,\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=6,\n",
    "        per_device_eval_batch_size=4,\n",
    "        log_level=\"debug\",\n",
    "        save_steps=100,\n",
    "        logging_steps=25, \n",
    "        learning_rate=2e-4,\n",
    "        eval_steps=50,\n",
    "        optim='adamw_8bit',\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        weight_decay=0.1,\n",
    "        max_steps=MAX_STEPS,\n",
    "        warmup_ratio=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct the model trainer\n",
    "- Will train the model with TRL (Transformer Reinforcement Learning), with the SFT (Supervised Fine Tuning) trainer\n",
    "- Use the text column of the dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = test_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = MAX_SEQ_LENGTH,\n",
    "    tokenizer = tokenizer,\n",
    "    args = training_arguments,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show current memory stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show final memory and time stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference\n",
    "Infer from the model using the earlier defined Alpca prompt format, leaving response blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Continue the fibonnaci sequence for the next 10 numbers.\", # instruction\n",
    "        \"1, 1, 2, 3, 5, 8\", # input\n",
    "        \"\", # response, leaving blank for generation!\n",
    "    )\n",
    "]*1, return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 128, use_cache = True)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference with text streamer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Continue the fibonnaci sequence for the next 10 numbers.\", # instruction\n",
    "        \"1, 1, 2, 3, 5, 8\", # input\n",
    "        \"\", # response, leaving blank for generation!\n",
    "    )\n",
    "]*1, return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving only the LoRA adapters and NOT the full model\n",
    "Using Huggingface's push_to_hub for an online save or save_pretrained for a local save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"mistral_lora_model\") # local saving\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Huggingface hub Online saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example saving to float16 for VLLM\n",
    "Be aware if I save here, then it merges into n-bit then clears the LoRAs, so will get a NoneType error if later saving the save_pretrained_gguf format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge to 16bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GGUF / llama.cpp Conversion\n",
    "Unsloth provides native GGUF/llama.cpp save. Clones llama.cpp and default save it to q8_0. Other quants include q4_k_m. Use save_pretrained_gguf for local saving and push_to_hub_gguf for uploading to HF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "# Save to 8bit Q8_0\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if True: model.save_pretrained_gguf(\"model_q4_k_m_gguf\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, use the model-unsloth.gguf file or model-unsloth-Q4_K_M.gguf file in llama.cpp or a UI based system like GPT4All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "model = PeftModel.from_pretrained(model, \"mistral_lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def get_response(query:str, input=\"\")->List[str]:\n",
    "  inputs = tokenizer(\n",
    "       [\n",
    "    alpaca_prompt.format(\n",
    "        query, # instruction\n",
    "        input, # input\n",
    "        \"\", # output\n",
    "    )\n",
    "    ]*1, return_tensors = \"pt\").to(\"cuda\")\n",
    "  outputs = model.generate(**inputs, max_new_tokens = 1024, use_cache = True)\n",
    "  return tokenizer.batch_decode(outputs)\n",
    "\n",
    "query = \"Give me some highlights of the following city\"\n",
    "input = \"Paris\"\n",
    "resp = get_response(query, input)\n",
    "def format_msg(message):\n",
    "    split_msg = message.split(\"### \")\n",
    "    final_str = split_msg[1]+split_msg[3]\n",
    "    return final_str\n",
    "print(format_msg(resp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.gptq import GPTQQuantizer, load_quantized_model\n",
    "quantizer = GPTQQuantizer(bits=4, dataset=\"c4\", block_name_to_quantize = \"model.decoder.layers\", model_seqlen = MAX_SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = \"./quantized_test/\"\n",
    "quantizer.save(model,save_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
