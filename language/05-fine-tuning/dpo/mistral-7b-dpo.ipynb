{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c15325e3-e837-42d5-a2ba-ccbdb9cd0622",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2efa664-dfe0-405e-8257-07eb47009e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#!pip install -q datasets trl peft bitsandbytes sentencepiece wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20d2efd8-1b5f-49e1-9550-511336761ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import DPOTrainer\n",
    "import bitsandbytes as bnb\n",
    "import wandb\n",
    "import os\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import Layout\n",
    "from datetime import datetime\n",
    "from jinja2 import Template\n",
    "from huggingface_hub import ModelCard, HfApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7992025-2efd-49fa-9706-87c7bd121c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb5fb9ee5dd946fb9798e519ad70cd26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='CorticalStack', description='HF hub user', style=TextStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5253c4699fe447d4bb8be32e6afb50f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='teknium/OpenHermes-2.5-Mistral-7B', description='Ref model id', style=TextStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b8c4a1da28b42fb95dfd379f028efdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='mistral-7b-neuralhermes-2.5-dpo', description='New model id', style=TextStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c05e88f5f8cf45beadc33d859bfaaca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='Intel/orca_dpo_pairs', description='DPO dataset', style=TextStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4acf2a29d7e64fbabf5e4555747dfdff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='apache-2.0', description='License', style=TextStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "username = widgets.Text(\n",
    "    value=\"CorticalStack\",\n",
    "    description='HF hub user',\n",
    "    disabled=False\n",
    ")\n",
    "username.style.description_width = 'initial'\n",
    "display(username)\n",
    "\n",
    "ref_model_id = widgets.Text(\n",
    "    value=\"teknium/OpenHermes-2.5-Mistral-7B\",\n",
    "    description='Ref model id',\n",
    "    disabled=False\n",
    ")\n",
    "ref_model_id.style.description_width = 'initial'\n",
    "display(ref_model_id)\n",
    "\n",
    "new_model_id = widgets.Text(\n",
    "    value=\"mistral-7b-neuralhermes-2.5-dpo\",\n",
    "    description='New model id',\n",
    "    disabled=False\n",
    ")\n",
    "new_model_id.style.description_width = 'initial'\n",
    "display(new_model_id)\n",
    "\n",
    "dpo_dataset = widgets.Text(\n",
    "    value=\"Intel/orca_dpo_pairs\",\n",
    "    description='DPO dataset',\n",
    "    disabled=False\n",
    ")\n",
    "dpo_dataset.style.description_width = 'initial'\n",
    "display(dpo_dataset)\n",
    "\n",
    "license = widgets.Text(\n",
    "    value=\"apache-2.0\",\n",
    "    description='License',\n",
    "    disabled=False\n",
    ")\n",
    "license.style.description_width = 'initial'\n",
    "display(license)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef306d2d-f8c1-4592-a683-d6368f5ed91d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f0873b06124a838682b3cc8902399d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=16, description='r', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c5062596b2d4a45823ff5d95ac58833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=16, description='lora alpha', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0731327615a34e61886605a77af0e36c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.05, description='lora dropout', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r = widgets.IntText(\n",
    "    value=16,\n",
    "    description='r',\n",
    "    disabled=False\n",
    ")\n",
    "r.style.description_width = 'initial'\n",
    "display(r)\n",
    "\n",
    "lora_alpha = widgets.IntText(\n",
    "    value=16,\n",
    "    description='lora alpha',\n",
    "    disabled=False\n",
    ")\n",
    "lora_alpha.style.description_width = 'initial'\n",
    "display(lora_alpha)\n",
    "\n",
    "lora_dropout = widgets.FloatText(\n",
    "    value=0.05,\n",
    "    description='lora dropout',\n",
    "    disabled=False\n",
    ")\n",
    "lora_dropout.style.description_width = 'initial'\n",
    "display(lora_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c47bc2f7-a659-479d-a524-75ff63c13332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f45a0c4284d44920bcc4f8c04246dc1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=100, description='Max steps', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2447705e9834d75a340f46f7d290540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='cosine', description='LR schedule type', style=TextStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7da88e41b26f42b4960dc39cea917803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.01, description='Warmup ratio', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7530f5ba2fc44f7c880dcb962a600aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='paged_adamw_32bit', description='Optimizer', style=TextStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "709c9d2882b24eec8a4bbb5a56e27cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=5e-05, description='Learning rate', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b25e11eea364cd881790c212e087924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=4, description='Per device train batch size', style=DescriptionStyle(description_width='initial'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0efb4dadf9fe44b19793180f171bb626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=4, description='Gradient accumulation steps', style=DescriptionStyle(description_width='initial'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4396db44aa3d426ebf52c2aacdc0e16b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=1024, description='Max prompt length', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "136994f9dfd04f0b8f3078e5a7a2d51f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=1536, description='Max length', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df282b5d733342b4801942cd013754d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.1, description='Beta', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_steps = widgets.IntText(\n",
    "    value=100,\n",
    "    description='Max steps',\n",
    "    disabled=False\n",
    ")\n",
    "max_steps.style.description_width = 'initial'\n",
    "display(max_steps)\n",
    "\n",
    "lr_scheduler_type = widgets.Text(\n",
    "    value=\"cosine\",\n",
    "    description='LR schedule type',\n",
    "    disabled=False\n",
    ")\n",
    "lr_scheduler_type.style.description_width = 'initial'\n",
    "display(lr_scheduler_type)\n",
    "\n",
    "warmup_ratio = widgets.FloatText(\n",
    "    value=0.01,\n",
    "    description='Warmup ratio',\n",
    "    disabled=False\n",
    ")\n",
    "warmup_ratio.style.description_width = 'initial'\n",
    "display(warmup_ratio)\n",
    "\n",
    "optim = widgets.Text(\n",
    "    value=\"paged_adamw_32bit\",\n",
    "    description='Optimizer',\n",
    "    disabled=False\n",
    ")\n",
    "optim.style.description_width = 'initial'\n",
    "display(optim)\n",
    "\n",
    "learning_rate = widgets.FloatText(\n",
    "    value=\"5e-5\",\n",
    "    description='Learning rate',\n",
    "    disabled=False\n",
    ")\n",
    "learning_rate.style.description_width = 'initial'\n",
    "display(learning_rate)\n",
    "\n",
    "per_device_train_batch_size = widgets.IntText(\n",
    "    value=4,\n",
    "    description='Per device train batch size',\n",
    "    disabled=False\n",
    ")\n",
    "per_device_train_batch_size.style.description_width = 'initial'\n",
    "display(per_device_train_batch_size)\n",
    "\n",
    "gradient_accumulation_steps = widgets.IntText(\n",
    "    value=4,\n",
    "    description='Gradient accumulation steps',\n",
    "    disabled=False\n",
    ")\n",
    "gradient_accumulation_steps.style.description_width = 'initial'\n",
    "display(gradient_accumulation_steps)\n",
    "\n",
    "max_prompt_length = widgets.IntText(\n",
    "    value=1024,\n",
    "    description='Max prompt length',\n",
    "    disabled=False\n",
    ")\n",
    "max_prompt_length.style.description_width = 'initial'\n",
    "display(max_prompt_length)\n",
    "\n",
    "max_length = widgets.IntText(\n",
    "    value=1536,\n",
    "    description='Max length',\n",
    "    disabled=False\n",
    ")\n",
    "max_length.style.description_width = 'initial'\n",
    "display(max_length)\n",
    "\n",
    "beta = widgets.FloatText(\n",
    "    value=\"0.1\",\n",
    "    description='Beta',\n",
    "    disabled=False\n",
    ")\n",
    "beta.style.description_width = 'initial'\n",
    "display(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9faf4b6c-6bed-4a26-b480-2fc235dbbe75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcorticalstack\u001b[0m (\u001b[33mcorticalstackteam\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "wandb.login()\n",
    "\n",
    "wandb_project = \"ft-\" + new_model_id.value\n",
    "if len(wandb_project) > 0:\n",
    "    os.environ[\"WANDB_PROJECT\"] = wandb_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab21c301-5c9d-441b-bfc1-a766b688f86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'chosen': 'Midsummer House is a moderately priced Chinese restaurant with a 3/5 customer rating, located near All Bar One.<|im_end|>\\n',\n",
       " 'rejected': ' Sure! Here\\'s a sentence that describes all the data you provided:\\n\\n\"Midsummer House is a moderately priced Chinese restaurant with a customer rating of 3 out of 5, located near All Bar One, offering a variety of delicious dishes.\"<|im_end|>\\n',\n",
       " 'prompt': '<|im_start|>system\\nYou are an AI assistant. You will be given a task. You must generate a detailed and long answer.<|im_end|>\\n<|im_start|>user\\nGenerate an approximately fifteen-word sentence that describes all this data: Midsummer House eatType restaurant; Midsummer House food Chinese; Midsummer House priceRange moderate; Midsummer House customer rating 3 out of 5; Midsummer House near All Bar One<|im_end|>\\n<|im_start|>assistant\\n'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chatml_format(example):\n",
    "    # Format system\n",
    "    if len(example['system']) > 0:\n",
    "        message = {\"role\": \"system\", \"content\": example['system']}\n",
    "        system = tokenizer.apply_chat_template([message], tokenize=False)\n",
    "    else:\n",
    "        system = \"\"\n",
    "\n",
    "    # Format instruction\n",
    "    message = {\"role\": \"user\", \"content\": example['question']}\n",
    "    prompt = tokenizer.apply_chat_template([message], tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    # Format chosen answer\n",
    "    chosen = example['chosen'] + \"<|im_end|>\\n\"\n",
    "\n",
    "    # Format rejected answer\n",
    "    rejected = example['rejected'] + \"<|im_end|>\\n\"\n",
    "\n",
    "    return {\n",
    "        \"prompt\": system + prompt,\n",
    "        \"chosen\": chosen,\n",
    "        \"rejected\": rejected,\n",
    "    }\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(f\"{dpo_dataset.value}\")['train']\n",
    "\n",
    "# Save columns\n",
    "original_columns = dataset.column_names\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"{ref_model_id.value}\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# Format dataset\n",
    "dataset = dataset.map(\n",
    "    chatml_format,\n",
    "    remove_columns=original_columns\n",
    ")\n",
    "\n",
    "# Print sample\n",
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c14588d2-4b35-4b5e-8642-b3d23c440b1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a6c9de27854ffd8ba8dcf6cf15d56b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06b486cb86704a56bbd1865783d7d0d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/dpo_trainer.py:328: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=r.value,\n",
    "    lora_alpha=lora_alpha.value,\n",
    "    lora_dropout=lora_dropout.value,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['k_proj', 'gate_proj', 'v_proj', 'up_proj', 'q_proj', 'o_proj', 'down_proj']\n",
    ")\n",
    "\n",
    "# Model to fine-tune\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    ref_model_id.value,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Reference model\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "    ref_model_id.value,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=per_device_train_batch_size.value,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps.value,\n",
    "    gradient_checkpointing=True,\n",
    "    learning_rate=learning_rate.value,\n",
    "    lr_scheduler_type=lr_scheduler_type.value,\n",
    "    max_steps=max_steps.value,\n",
    "    save_strategy=\"no\",\n",
    "    logging_steps=1,\n",
    "    output_dir=new_model_id.value,\n",
    "    optim=optim.value,\n",
    "    warmup_ratio = warmup_ratio.value,\n",
    "    bf16=True,\n",
    "    report_to=\"wandb\",\n",
    ")\n",
    "\n",
    "# Create DPO trainer\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model,\n",
    "    #ref_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=peft_config,\n",
    "    beta=beta.value,\n",
    "    max_prompt_length=max_prompt_length.value,\n",
    "    max_length=max_length.value,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daecdb07-e04c-4de7-9eac-c94fcbe6cb07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240218_220942-a2xf4agb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/corticalstackteam/ft-mistral-7b-neuralhermes-2.5-dpo/runs/a2xf4agb' target=\"_blank\">enchanting-fuse-2</a></strong> to <a href='https://wandb.ai/corticalstackteam/ft-mistral-7b-neuralhermes-2.5-dpo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/corticalstackteam/ft-mistral-7b-neuralhermes-2.5-dpo' target=\"_blank\">https://wandb.ai/corticalstackteam/ft-mistral-7b-neuralhermes-2.5-dpo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/corticalstackteam/ft-mistral-7b-neuralhermes-2.5-dpo/runs/a2xf4agb' target=\"_blank\">https://wandb.ai/corticalstackteam/ft-mistral-7b-neuralhermes-2.5-dpo/runs/a2xf4agb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 33:15, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.081700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.040900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.126600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.130600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.033100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.014500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.018100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.119700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.068200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.007600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.046700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.103600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.028200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.073100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.010500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.025100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.006600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.004200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.049600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.006600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.004300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.011500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.006800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.001900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=0.024510806764007177, metrics={'train_runtime': 2014.7381, 'train_samples_per_second': 0.794, 'train_steps_per_second': 0.05, 'total_flos': 0.0, 'train_loss': 0.024510806764007177, 'epoch': 0.12})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tune model with DPO\n",
    "dpo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8b5714a-da1f-4f25-8902-5478b5ddd661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c99df69933a4737aa37189de514b184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('mistral-7b-neuralhermes-2.5-dpo/tokenizer_config.json',\n",
       " 'mistral-7b-neuralhermes-2.5-dpo/special_tokens_map.json',\n",
       " 'mistral-7b-neuralhermes-2.5-dpo/tokenizer.model',\n",
       " 'mistral-7b-neuralhermes-2.5-dpo/added_tokens.json',\n",
       " 'mistral-7b-neuralhermes-2.5-dpo/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save artifacts\n",
    "dpo_trainer.model.save_pretrained(\"final_checkpoint\")\n",
    "tokenizer.save_pretrained(\"final_checkpoint\")\n",
    "\n",
    "# Flush memory\n",
    "del dpo_trainer, model, ref_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Reload model in FP16 (instead of NF4)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    ref_model_id.value,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(ref_model_id.value)\n",
    "\n",
    "# Merge base model with the adapter\n",
    "model = PeftModel.from_pretrained(base_model, \"final_checkpoint\")\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_pretrained(new_model_id.value)\n",
    "tokenizer.save_pretrained(new_model_id.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "504a9504-c0d5-4277-8801-4fa00a29431e",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_text = \"\"\"\n",
    "---\n",
    "license: {{ license }}\n",
    "tags:\n",
    "- dpo\n",
    "dataset:\n",
    "- {{ train_dataset_name }}\n",
    "base_model:\n",
    "- {{ model_id }}\n",
    "---\n",
    "\n",
    "# {{ new_model_id }}\n",
    "\n",
    "{{ new_model_id }} is a DPO fine-tuned version of [{{ model_id }}](https://huggingface.co/{{ model_id }}) using the [{{ train_dataset_name }}](https://huggingface.co/datasets/{{ train_dataset_name }}) dataset.\n",
    "\n",
    "### LoRA\n",
    "- r: {{ r }}\n",
    "- LoRA alpha: {{ lora_alpha }}\n",
    "- LoRA dropout: {{ lora_dropout }}\n",
    "\n",
    "### Training arguments\n",
    "- Batch size: {{ per_device_train_batch_size }}\n",
    "- Gradient accumulation steps: {{ gradient_accumulation_steps }}\n",
    "- Optimizer: {{ optim }}\n",
    "- Max steps: {{ max_steps }}\n",
    "- Learning rate: {{ learning_rate }}\n",
    "- Learning rate scheduler type: {{ lr_scheduler_type }}\n",
    "- Beta: {{ beta }}\n",
    "- Max prompt length: {{ max_prompt_length }}\n",
    "- Max length: {{ max_length }}\n",
    "\"\"\"\n",
    "\n",
    "    # Create a Jinja template object\n",
    "jinja_template = Template(template_text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08c37dbd-9de1-48af-b6c2-04029a9bf06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the template\n",
    "content = jinja_template.render(\n",
    "          license = license.value,\n",
    "          train_dataset_name = dpo_dataset.value,\n",
    "          model_id = ref_model_id.value,\n",
    "          new_model_id = new_model_id.value,\n",
    "          r = r.value,\n",
    "          lora_alpha = lora_alpha.value,\n",
    "          lora_dropout = lora_dropout.value,\n",
    "          per_device_train_batch_size = per_device_train_batch_size.value,\n",
    "          gradient_accumulation_steps = gradient_accumulation_steps.value,\n",
    "          optim = optim.value,\n",
    "          max_steps = max_steps.value,\n",
    "          learning_rate = learning_rate.value,\n",
    "          lr_scheduler_type = lr_scheduler_type.value,\n",
    "          beta = beta.value,\n",
    "          max_prompt_length = max_prompt_length.value,\n",
    "          max_length = max_length.value\n",
    "          )\n",
    "\n",
    "# Save the model card\n",
    "card = ModelCard(content)\n",
    "card.save(f\"{new_model_id.value}/README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f4d4742-6482-475a-9aa7-b44cb8afd610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/CorticalStack/mistral-7b-neuralhermes-2.5-dpo', endpoint='https://huggingface.co', repo_type='model', repo_id='CorticalStack/mistral-7b-neuralhermes-2.5-dpo')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api = HfApi(token=HF_TOKEN)\n",
    "api.create_repo(\n",
    "    repo_id=f\"{username.value}/{new_model_id.value}\",\n",
    "    repo_type=\"model\",\n",
    "    exist_ok=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "feba735a-1f25-412f-9aa6-15126f7f42a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1ebd1dc178449dd9ae20f7c79834e4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "356b5304233f4eeb9230666214ec0359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e102ef8e19ff4d69993f5c68340f01b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0da132253b8a46b99bd0a42dae3b37c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88c2412625db42c3a18db98a15440298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/CorticalStack/mistral-7b-neuralhermes-2.5-dpo/commit/282787d4374fbc17a6266e14e670e9ae7893cf33', commit_message='Upload folder using huggingface_hub', commit_description='', oid='282787d4374fbc17a6266e14e670e9ae7893cf33', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.upload_folder(\n",
    "    repo_id=f\"{username.value}/{new_model_id.value}\",\n",
    "    folder_path=new_model_id.value,\n",
    "    token=HF_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "517b265e-ecce-4808-b56a-eb3a543ad662",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'transformers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m prompt \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(message, add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Create pipeline\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mtransformers\u001b[49m\u001b[38;5;241m.\u001b[39mpipeline(\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     model\u001b[38;5;241m=\u001b[39mnew_model_id\u001b[38;5;241m.\u001b[39mvalue,\n\u001b[1;32m     13\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Generate text\u001b[39;00m\n\u001b[1;32m     17\u001b[0m sequences \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[1;32m     18\u001b[0m     prompt,\n\u001b[1;32m     19\u001b[0m     do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,\n\u001b[1;32m     24\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transformers' is not defined"
     ]
    }
   ],
   "source": [
    "# Format prompt\n",
    "message = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant chatbot.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is a Large Language Model?\"}\n",
    "]\n",
    "tokenizer = AutoTokenizer.from_pretrained(new_model_id.value)\n",
    "prompt = tokenizer.apply_chat_template(message, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=new_model_id.value,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Generate text\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    num_return_sequences=1,\n",
    "    max_length=200,\n",
    ")\n",
    "print(sequences[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e7bbff-d4dc-4287-a42a-be1470b19d89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
