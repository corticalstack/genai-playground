{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de2b47e0-6e21-40a8-a08c-30fe0998bd3d",
   "metadata": {},
   "source": [
    "### Import dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2602c0-fa4b-458f-97f4-46376938fe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e709d3a-5941-4783-a65d-f58c287c81d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b14a7ac-2311-48f2-904c-0888c8016379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q -U \"transformers==4.38.0\" --upgrade\n",
    "# !pip install -q datasets peft trl accelerate\n",
    "# !pip install -i https://pypi.org/simple/ bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf97e9e-78b1-4abe-a202-eb9cde5550ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, HfArgumentParser, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d69c93-0f87-42f4-abcc-b00aaddf504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import Layout\n",
    "import os\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "from jinja2 import Template\n",
    "from huggingface_hub import ModelCard, HfApi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d25b649-2b1c-4376-bbd6-d99c991a32eb",
   "metadata": {},
   "source": [
    "### Huggingface Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a82b31-10ef-42f4-8ed4-0ab00468cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = widgets.Text(\n",
    "    value=\"CorticalStack\",\n",
    "    description='HF hub user',\n",
    "    disabled=False\n",
    ")\n",
    "username.style.description_width = 'initial'\n",
    "display(username)\n",
    "\n",
    "model_id = widgets.Text(\n",
    "    value=\"google/gemma-7b\",\n",
    "    description='Model id',\n",
    "    disabled=False\n",
    ")\n",
    "model_id.style.description_width = 'initial'\n",
    "display(model_id)\n",
    "\n",
    "new_model_id = widgets.Text(\n",
    "    value=\"gemma-7b\",\n",
    "    description='New model id',\n",
    "    disabled=False\n",
    ")\n",
    "new_model_id.style.description_width = 'initial'\n",
    "display(new_model_id)\n",
    "\n",
    "license = widgets.Text(\n",
    "    value=\"apache-2.0\",\n",
    "    description='License',\n",
    "    disabled=False\n",
    ")\n",
    "license.style.description_width = 'initial'\n",
    "display(license)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c849a081-a145-45ae-95e3-9e3b7d518992",
   "metadata": {},
   "source": [
    "### Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff6442d-fc82-45cd-b895-3cbcdeb6a84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = widgets.Text(\n",
    "    value=\"stingning/ultrachat\",\n",
    "    description='Training dataset',\n",
    "    disabled=False\n",
    ")\n",
    "training_dataset.style.description_width = 'initial'\n",
    "display(training_dataset)\n",
    "\n",
    "per_device_train_batch_size = widgets.IntText(\n",
    "    value=4,\n",
    "    description='Per device train batch size',\n",
    "    disabled=False\n",
    ")\n",
    "per_device_train_batch_size.style.description_width = 'initial'\n",
    "display(per_device_train_batch_size)\n",
    "\n",
    "gradient_accumulation_steps = widgets.IntText(\n",
    "    value=6,\n",
    "    description='Gradient accumulation steps',\n",
    "    disabled=False\n",
    ")\n",
    "gradient_accumulation_steps.style.description_width = 'initial'\n",
    "display(gradient_accumulation_steps)\n",
    "\n",
    "learning_rate = widgets.FloatText(\n",
    "    value=\"2e-4\",\n",
    "    description='Learning rate',\n",
    "    disabled=False\n",
    ")\n",
    "learning_rate.style.description_width = 'initial'\n",
    "display(learning_rate)\n",
    "\n",
    "max_grad_norm = widgets.FloatText(\n",
    "    value=\"0.3\",\n",
    "    description='Max grad norm',\n",
    "    disabled=False\n",
    ")\n",
    "max_grad_norm.style.description_width = 'initial'\n",
    "display(max_grad_norm)\n",
    "\n",
    "weight_decay = widgets.FloatText(\n",
    "    value=\"0.001\",\n",
    "    description='Weight decay',\n",
    "    disabled=False\n",
    ")\n",
    "weight_decay.style.description_width = 'initial'\n",
    "display(weight_decay)\n",
    "\n",
    "lora_alpha = widgets.IntText(\n",
    "    value=16,\n",
    "    description='lora alpha',\n",
    "    disabled=False\n",
    ")\n",
    "lora_alpha.style.description_width = 'initial'\n",
    "display(lora_alpha)\n",
    "\n",
    "lora_dropout = widgets.FloatText(\n",
    "    value=0.1,\n",
    "    description='lora dropout',\n",
    "    disabled=False\n",
    ")\n",
    "lora_dropout.style.description_width = 'initial'\n",
    "display(lora_dropout)\n",
    "\n",
    "lora_r = widgets.IntText(\n",
    "    value=8,\n",
    "    description='LoRA r',\n",
    "    disabled=False\n",
    ")\n",
    "lora_r.style.description_width = 'initial'\n",
    "display(lora_r)\n",
    "\n",
    "max_seq_length = widgets.IntText(\n",
    "    value=2048, \n",
    "    description='Max seq length',\n",
    "    disabled=False\n",
    ")\n",
    "max_seq_length.style.description_width = 'initial'\n",
    "display(max_seq_length)\n",
    "\n",
    "fp16 = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='BF16 training enabled',\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "fp16.style.description_width = 'initial'\n",
    "display(fp16)\n",
    "\n",
    "bf16 = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='BF16 training enabled',\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "bf16.style.description_width = 'initial'\n",
    "display(bf16)\n",
    "\n",
    "packing = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Use packing dataset creating',\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "packing.style.description_width = 'initial'\n",
    "display(packing)\n",
    "\n",
    "gradient_checkpointing= widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Enable gradient checkpointing',\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "gradient_checkpointing.style.description_width = 'initial'\n",
    "display(gradient_checkpointing)\n",
    "\n",
    "use_flash_attn_2 = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Use flash attention 2',\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "use_flash_attn_2.style.description_width = 'initial'\n",
    "display(use_flash_attn_2)\n",
    "\n",
    "optim = widgets.Text(\n",
    "    value=\"paged_adamw_32bit\",\n",
    "    description='Optimizer',\n",
    "    disabled=False\n",
    ")\n",
    "optim.style.description_width = 'initial'\n",
    "display(optim)\n",
    "\n",
    "lr_scheduler_type = widgets.Text(\n",
    "    value=\"constant\",\n",
    "    description='LR schedule type',\n",
    "    disabled=False\n",
    ")\n",
    "lr_scheduler_type.style.description_width = 'initial'\n",
    "display(lr_scheduler_type)\n",
    "\n",
    "max_steps = widgets.IntText(\n",
    "    value=100,\n",
    "    description='Max number of optimizer update steps',\n",
    "    disabled=False\n",
    ")\n",
    "max_steps.style.description_width = 'initial'\n",
    "display(max_steps)\n",
    "\n",
    "warmup_ratio = widgets.FloatText(\n",
    "    value=0.01,\n",
    "    description='Warmup ratios',\n",
    "    disabled=False\n",
    ")\n",
    "warmup_ratio.style.description_width = 'initial'\n",
    "display(warmup_ratio)\n",
    "\n",
    "eval_steps = widgets.IntText(\n",
    "    value=10,\n",
    "    description='Eval steps',\n",
    "    disabled=False\n",
    ")\n",
    "eval_steps.style.description_width = 'initial'\n",
    "display(eval_steps)\n",
    "\n",
    "save_steps = widgets.IntText(\n",
    "    value=50,\n",
    "    description='Save steps',\n",
    "    disabled=False\n",
    ")\n",
    "save_steps.style.description_width = 'initial'\n",
    "display(save_steps)\n",
    "\n",
    "logging_steps = widgets.IntText(\n",
    "    value=1,\n",
    "    description='Logging steps',\n",
    "    disabled=False\n",
    ")\n",
    "logging_steps.style.description_width = 'initial'\n",
    "display(logging_steps)\n",
    "\n",
    "random_state = widgets.IntText(\n",
    "    value=3407,\n",
    "    description='random state',\n",
    "    disabled=False\n",
    ")\n",
    "random_state.style.description_width = 'initial'\n",
    "display(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89765192-9e73-4d7e-a6f6-741f69d1441f",
   "metadata": {},
   "source": [
    "### Tracking training with weights & biases (wandb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768d597b-2237-49e2-84b0-a646fd34fa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "\n",
    "wandb_project = \"ft-\" + new_model_id.value\n",
    "if len(wandb_project) > 0:\n",
    "    os.environ[\"WANDB_PROJECT\"] = wandb_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881b7b04-19e4-4b2a-a50d-3c3ccbca8a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(training_dataset.value, split='train[:25%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9997b523-2283-433d-952c-11cc2edcb87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_ratio = 0.99  # 80% train, 20% test\n",
    "split_dataset = dataset.train_test_split(test_size=1-train_test_ratio)\n",
    "train_dataset = split_dataset['train']\n",
    "test_dataset = split_dataset['test']\n",
    "print(f\"train dataset records: {len(train_dataset)}\")\n",
    "print(f\"test dataset records: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efde3835-a535-4cab-a334-1586783af14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "train_test_split = dataset[\"train\"].train_test_split(test_size=0.001)\n",
    "\n",
    "# Extract the training and testing datasets\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_dataset = train_test_split[\"test\"]\n",
    "\n",
    "print(f\"train dataset records: {len(train_dataset)}\")\n",
    "print(f\"test dataset records: {len(test_dataset)}\")\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de15da7-a1b2-43d6-a6cc-698b242ee3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"### USER: {}\\n### ASSISTANT: {}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c602140-c169-46e8-8b10-24c84704bd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompts(examples):\n",
    "    texts = []\n",
    "    for convo in examples[\"data\"]:\n",
    "        input = convo[0]\n",
    "        response = convo[1]\n",
    "        text = prompt.format(input, response)\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9b4f80-23d8-4c47-af06-27cca6f4131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_in_prompt_format = train_dataset.map(format_prompts, batched = True,)\n",
    "test_dataset_in_prompt_format = test_dataset.map(format_prompts, batched = True,)\n",
    "\n",
    "print(train_dataset_in_prompt_format[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229ecea9-e2d8-439d-9b4d-3bf1c3515049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d0798d-e340-40d8-ba95-fe4a8c78e702",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d760bb-a463-4aec-a5be-25bacf64d919",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id.value, \n",
    "    quantization_config=quantization_config, \n",
    "    torch_dtype=torch.float32,\n",
    "    attn_implementation=\"sdpa\" if not use_flash_attn_2.value else \"flash_attention_2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8197d3c-bb9e-41ae-9ea2-371df089395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id.value)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e222f196-276e-4257-8f3f-4264846c6919",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=lora_r.value,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_alpha=lora_alpha.value,\n",
    "    lora_dropout=lora_dropout.value\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc66bcb0-3cf4-4fcf-baa2-1cc3e13009d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"{username.value}/gemma-7b-ultrachat-sft\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcef6164-ae51-45ec-be57-d9e692ce0a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "HAS_BFLOAT16 = torch.cuda.is_bf16_supported()\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size.value,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps.value,\n",
    "    optim=optim.value,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps = eval_steps.value,  \n",
    "    report_to = \"wandb\", \n",
    "    run_name = f\"{wandb_project}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\",\n",
    "    save_steps=save_steps.value,\n",
    "    logging_steps=logging_steps.value,\n",
    "    learning_rate=learning_rate.value,\n",
    "    max_grad_norm=max_grad_norm.value,\n",
    "    max_steps=max_steps.value,\n",
    "    warmup_ratio=warmup_ratio.value,\n",
    "    lr_scheduler_type=lr_scheduler_type.value,\n",
    "    gradient_checkpointing=gradient_checkpointing.value,\n",
    "    fp16 = not HAS_BFLOAT16,\n",
    "    bf16 = HAS_BFLOAT16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ff9c38-600b-4caf-862f-9a96c0577c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset = train_dataset_in_prompt_format,\n",
    "    eval_dataset = test_dataset_in_prompt_format,\n",
    "    args=training_arguments,\n",
    "    peft_config=lora_config,\n",
    "    packing=packing.value,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length.value,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f7544b-7509-4297-b500-a112bc0dc478",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3effd2a9-20f5-4dad-b03e-11c53cc0ca1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17138416-eb0e-4e45-9a3b-84a7ec08c9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce121892-8539-4ec5-9f2b-23034cec297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8f7d27-c766-4dc4-b69c-e27e4949ac67",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = \"gemma-7b-ultrachat-sft\" #Name of the model you will be pushing to huggingface model hub\n",
    "# Save the fine-tuned model\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ccdc63-cc5b-4244-8eb2-71a68c9eb6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a4c42b-4427-4551-9f3a-b69f836a8d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the model with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id.value,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "merged_model= PeftModel.from_pretrained(base_model, new_model)\n",
    "merged_model= merged_model.merge_and_unload()\n",
    "\n",
    "# Save the merged model\n",
    "merged_model.save_pretrained(\"merged_model\",safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"merged_model\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5140f476-6759-4469-855a-f11ddf5c9644",
   "metadata": {},
   "source": [
    "### Create HF model card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c773ad-0ccf-41ef-b0c3-ba9591bf6044",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_text = \"\"\"\n",
    "---\n",
    "license: {{ license }}\n",
    "---\n",
    "\n",
    "# {{ new_model_id }}\n",
    "\n",
    "{{ new_model_id }} is an SFT fine-tuned version of [{{ model_id }}](https://huggingface.co/{{ model_id }}) using the [{{ train_dataset_name }}](https://huggingface.co/datasets/{{ train_dataset_name }}) dataset.\n",
    "\n",
    "## Fine-tuning configuration\n",
    "### LoRA\n",
    "- LoRA r: {{ r }}\n",
    "- LoRA alpha: {{ lora_alpha }}\n",
    "- LoRA dropout: {{ lora_dropout }}\n",
    "\n",
    "### Training arguments\n",
    "- Epochs: {{ num_epochs }}\n",
    "- Batch size: {{ per_device_train_batch_size }}\n",
    "- Gradient accumulation steps: {{ gradient_accumulation_steps }}\n",
    "- Optimizer: {{ optim }}\n",
    "- Max steps: {{ max_steps }}\n",
    "- Learning rate: {{ learning_rate }}\n",
    "- Weight decay: {{ weight_decay }}\n",
    "- Learning rate scheduler type: {{ lr_scheduler_type }}\n",
    "- Max seq length: {{ max_seq_length }}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # Create a Jinja template object\n",
    "jinja_template = Template(template_text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8800504e-e7fe-4976-9b3d-6ad712288d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the template\n",
    "content = jinja_template.render(\n",
    "          license = license.value,\n",
    "          new_model_id = new_model_id.value,\n",
    "          model_id = model_id.value,\n",
    "          train_dataset_name = training_dataset.value,\n",
    "          r = lora_r.value,\n",
    "          lora_alpha = lora_alpha.value,\n",
    "          lora_dropout = lora_dropout.value,\n",
    "          num_epochs = 1,\n",
    "          per_device_train_batch_size = per_device_train_batch_size.value,\n",
    "          gradient_accumulation_steps = gradient_accumulation_steps.value,\n",
    "          optim = optim.value,\n",
    "          max_steps = max_steps.value,\n",
    "          learning_rate = learning_rate.value,\n",
    "          weight_decay = weight_decay.value,\n",
    "          lr_scheduler_type = lr_scheduler_type.value,\n",
    "          max_seq_length = max_seq_length.value,\n",
    "          )\n",
    "\n",
    "# Save the model card\n",
    "card = ModelCard(content)\n",
    "card.save(\"merged_model/README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc548dc4-589d-4f86-b396-ae1af271244c",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = HfApi(token=HF_TOKEN)\n",
    "api.create_repo(\n",
    "    repo_id=f\"{username.value}/{new_model_id.value}\",\n",
    "    repo_type=\"model\",\n",
    "    exist_ok=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf6912f-e2f7-413c-9a43-30ebf470f779",
   "metadata": {},
   "outputs": [],
   "source": [
    "api.upload_folder(\n",
    "    repo_id=f\"{username.value}/{new_model_id.value}\",\n",
    "    folder_path=\"merged_model\",\n",
    "    token=HF_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab65649-302a-4b40-a489-4ddce2554ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_id.value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
