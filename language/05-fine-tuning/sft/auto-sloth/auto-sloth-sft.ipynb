{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import Layout\n",
    "import os\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "from jinja2 import Template\n",
    "from huggingface_hub import ModelCard, HfApi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set variables like HuggingFace token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()  # take environment variables from .env.\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huggingface Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1816bc0d6d704de49b4ddf718e78aff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='CorticalStack', description='HF hub user', style=TextStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d6b3da3c9504aeb97ac37934d2d149b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='mistral-7b-alpaca-sft', description='New model id', style=TextStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab666f8738cd46c9bfe118b375209570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='apache-2.0', description='License', style=TextStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "username = widgets.Text(\n",
    "    value=\"CorticalStack\",\n",
    "    description='HF hub user',\n",
    "    disabled=False\n",
    ")\n",
    "username.style.description_width = 'initial'\n",
    "display(username)\n",
    "\n",
    "new_model_id = widgets.Text(\n",
    "    value=\"mistral-7b-alpaca-sft\",\n",
    "    description='New model id',\n",
    "    disabled=False\n",
    ")\n",
    "new_model_id.style.description_width = 'initial'\n",
    "display(new_model_id)\n",
    "\n",
    "license = widgets.Text(\n",
    "    value=\"apache-2.0\",\n",
    "    description='License',\n",
    "    disabled=False\n",
    ")\n",
    "license.style.description_width = 'initial'\n",
    "display(license)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8832fc5323e444ffb17fb08e560890b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=256, description='r', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb5da4f4545649c480e7b96b51233672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=128, description='lora alpha', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab167839e2f14c1e979432892479e498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='lora dropout', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37ee595a87254cd4b4c1ceb2073afdc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=3407, description='random state', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r = widgets.IntText(\n",
    "    value=256,\n",
    "    description='r',\n",
    "    disabled=False\n",
    ")\n",
    "r.style.description_width = 'initial'\n",
    "display(r)\n",
    "\n",
    "lora_alpha = widgets.IntText(\n",
    "    value=128,\n",
    "    description='lora alpha',\n",
    "    disabled=False\n",
    ")\n",
    "lora_alpha.style.description_width = 'initial'\n",
    "display(lora_alpha)\n",
    "\n",
    "lora_dropout = widgets.FloatText(\n",
    "    value=0,\n",
    "    description='lora dropout',\n",
    "    disabled=False\n",
    ")\n",
    "lora_dropout.style.description_width = 'initial'\n",
    "display(lora_dropout)\n",
    "\n",
    "random_state = widgets.IntText(\n",
    "    value=3407,\n",
    "    description='random state',\n",
    "    disabled=False\n",
    ")\n",
    "random_state.style.description_width = 'initial'\n",
    "display(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11b796db16349a680eb01ba3ed4aa85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='unsloth/mistral-7b-bnb-4bit', description='Model ID', style=TextStyle(description_width='initial')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ea278dbfab44b39985493d4b439e37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='yahma/alpaca-cleaned', description='Train dataset', style=TextStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea44feee5f234b718d3deeead60d54ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=1, description='Number epochs', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2974244ba9a41499bfe42c40c70bb45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=4, description='Per device train batch size', style=DescriptionStyle(description_width='initial'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08db426b1d64876a9a3822d0ca0ca36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=4, description='Gradient accumulation steps', style=DescriptionStyle(description_width='initial'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7db5f3eb614364a277fd33fd3d816d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=4, description='Dataset num proc', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c08daededdc543cd9131f12fb0ec16fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=1, description='Logging steps', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55394ab62a37446cbf0dc6f95c7d1c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='adamw_torch_fused', description='Optimizer', style=TextStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eaebc669d6a41bdb338326ed0a64a37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=5, description='Warmup steps', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14454d90a3f3410cae83279ae32fefba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=30, description='Max steps', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1385e2f8f1e43a3ad8e64fe7bcabe1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=5, description='Eval steps', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f225d83172e942d3b2c54a7c16de6745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=5, description='Save steps', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ac67771c15d4088988761b396baee39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0002, description='Learning rate', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee6dcf346d44d088d4b76bba1a5653b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.01, description='Weight decay', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a549ac95d244f679ad2dbbb53bf85d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='linear', description='LR schedule type', style=TextStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd360a8dd43346bfb8dead7d552cc4b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=2048, description='Max seq length', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "348e8040599b49219d78888b34092bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='Four bit bnb', indent=False, style=CheckboxStyle(description_width='initial'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = widgets.Text(\n",
    "    value=\"unsloth/mistral-7b-bnb-4bit\",\n",
    "    description='Model ID',\n",
    "    disabled=False\n",
    ")\n",
    "model_id.style.description_width = 'initial'\n",
    "display(model_id)\n",
    "\n",
    "train_dataset_name = widgets.Text(\n",
    "    value=\"yahma/alpaca-cleaned\",\n",
    "    description='Train dataset',\n",
    "    disabled=False\n",
    ")\n",
    "train_dataset_name.style.description_width = 'initial'\n",
    "display(train_dataset_name)\n",
    "\n",
    "num_epochs = widgets.IntText(\n",
    "    value=1,\n",
    "    description='Number epochs',\n",
    "    disabled=False\n",
    ")\n",
    "num_epochs.style.description_width = 'initial'\n",
    "display(num_epochs)\n",
    "\n",
    "per_device_train_batch_size = widgets.IntText(\n",
    "    value=4,\n",
    "    description='Per device train batch size',\n",
    "    disabled=False\n",
    ")\n",
    "per_device_train_batch_size.style.description_width = 'initial'\n",
    "display(per_device_train_batch_size)\n",
    "\n",
    "gradient_accumulation_steps = widgets.IntText(\n",
    "    value=4,\n",
    "    description='Gradient accumulation steps',\n",
    "    disabled=False\n",
    ")\n",
    "gradient_accumulation_steps.style.description_width = 'initial'\n",
    "display(gradient_accumulation_steps)\n",
    "\n",
    "dataset_num_proc = widgets.IntText(\n",
    "    value=4,\n",
    "    description='Dataset num proc',\n",
    "    disabled=False\n",
    ")\n",
    "dataset_num_proc.style.description_width = 'initial'\n",
    "display(dataset_num_proc)\n",
    "\n",
    "logging_steps = widgets.IntText(\n",
    "    value=1,\n",
    "    description='Logging steps',\n",
    "    disabled=False\n",
    ")\n",
    "logging_steps.style.description_width = 'initial'\n",
    "display(logging_steps)\n",
    "\n",
    "optim = widgets.Text(\n",
    "    value=\"adamw_torch_fused\",\n",
    "    description='Optimizer',\n",
    "    disabled=False\n",
    ")\n",
    "optim.style.description_width = 'initial'\n",
    "display(optim)\n",
    "\n",
    "warmup_steps = widgets.IntText(\n",
    "    value=5,\n",
    "    description='Warmup steps',\n",
    "    disabled=False\n",
    ")\n",
    "warmup_steps.style.description_width = 'initial'\n",
    "display(warmup_steps)\n",
    "\n",
    "max_steps = widgets.IntText(\n",
    "    value=30,\n",
    "    description='Max steps',\n",
    "    disabled=False\n",
    ")\n",
    "max_steps.style.description_width = 'initial'\n",
    "display(max_steps)\n",
    "\n",
    "eval_steps = widgets.IntText(\n",
    "    value=5,\n",
    "    description='Eval steps',\n",
    "    disabled=False\n",
    ")\n",
    "eval_steps.style.description_width = 'initial'\n",
    "display(eval_steps)\n",
    "\n",
    "save_steps = widgets.IntText(\n",
    "    value=5,\n",
    "    description='Save steps',\n",
    "    disabled=False\n",
    ")\n",
    "save_steps.style.description_width = 'initial'\n",
    "display(save_steps)\n",
    "\n",
    "learning_rate = widgets.FloatText(\n",
    "    value=\"2e-4\",\n",
    "    description='Learning rate',\n",
    "    disabled=False\n",
    ")\n",
    "learning_rate.style.description_width = 'initial'\n",
    "display(learning_rate)\n",
    "\n",
    "weight_decay = widgets.FloatText(\n",
    "    value=\"0.01\",\n",
    "    description='Weight decay',\n",
    "    disabled=False\n",
    ")\n",
    "weight_decay.style.description_width = 'initial'\n",
    "display(weight_decay)\n",
    "\n",
    "lr_scheduler_type = widgets.Text(\n",
    "    value=\"linear\",\n",
    "    description='LR schedule type',\n",
    "    disabled=False\n",
    ")\n",
    "lr_scheduler_type.style.description_width = 'initial'\n",
    "display(lr_scheduler_type)\n",
    "\n",
    "max_seq_length = widgets.IntText(\n",
    "    value=2048,  #1024,\n",
    "    description='Max seq length',\n",
    "    disabled=False\n",
    ")\n",
    "max_seq_length.style.description_width = 'initial'\n",
    "display(max_seq_length)\n",
    "\n",
    "four_bit_bnb = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Four bit bnb',\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "four_bit_bnb.style.description_width = 'initial'\n",
    "display(four_bit_bnb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking training with weights & biases (wandb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcorticalstack\u001b[0m (\u001b[33mcorticalstackteam\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "wandb.login()\n",
    "\n",
    "wandb_project = \"ft-\" + new_model_id.value\n",
    "if len(wandb_project) > 0:\n",
    "    os.environ[\"WANDB_PROJECT\"] = wandb_project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset from the HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "781c6fda4102433ca28452c8d548879d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/4.45k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b56015e298a4a25a9cfb4a6e7b6d9dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/396M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e9e40175c2740bab824bf6c2bf7a13c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(train_dataset_name.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset records: 355500\n",
      "test dataset records: 39500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'original_question': \"Jane's quiz scores were 98, 97, 92, 85 and 93. What was her mean score?\",\n",
       " 'type': 'MATH_AnsAug',\n",
       " 'query': \"Jane's quiz scores were 98, 97, 92, 85 and 93. What was her mean score?\",\n",
       " 'response': \"The mean score is the sum of the scores divided by the number of scores.\\nThe sum of Jane's scores is $98 + 97 + 92 + 85 + 93 = 465$.\\nSince Jane has 5 scores, her mean score is $\\\\frac{465}{5} = \\\\boxed{93}$.\\nThe answer is: 93\"}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "train_test_split = dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "\n",
    "# Extract the training and testing datasets\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_dataset = train_test_split[\"test\"]\n",
    "\n",
    "print(f\"train dataset records: {len(train_dataset)}\")\n",
    "print(f\"test dataset records: {len(test_dataset)}\")\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Mistral patching release 2024.2\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.691 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.1.2.post301. CUDA = 8.6. CUDA Toolkit = 11.8.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.21. FA = True.\n",
      " \"-____-\"     Apache 2 free license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed `quantization_config` to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` attribute will be overwritten with the one you passed to `from_pretrained`.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_id.value,\n",
    "    max_seq_length = max_seq_length.value,\n",
    "    dtype = None, # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "    load_in_4bit = four_bit_bnb.value, # Use 4bit quantization to reduce memory usage. Can be False\n",
    "    token=HF_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set prompt - Alpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def format_prompts(examples):\n",
    "    instructions = examples[\"query\"]\n",
    "    #inputs       = examples[\"query\"]\n",
    "    outputs      = examples[\"response\"]\n",
    "    texts = []\n",
    "    for instruction, output in zip(instructions, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e25a854044d417ab4bdd90ff971f671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/355500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936c84f31b394c18a555795c55d5b143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/39500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Jane's quiz scores were 98, 97, 92, 85 and 93. What was her mean score?\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "The mean score is the sum of the scores divided by the number of scores.\n",
      "The sum of Jane's scores is $98 + 97 + 92 + 85 + 93 = 465$.\n",
      "Since Jane has 5 scores, her mean score is $\\frac{465}{5} = \\boxed{93}$.\n",
      "The answer is: 93</s>\n"
     ]
    }
   ],
   "source": [
    "train_dataset_in_prompt_format = train_dataset.map(format_prompts, batched = True,)\n",
    "test_dataset_in_prompt_format = test_dataset.map(format_prompts, batched = True,)\n",
    "print(train_dataset_in_prompt_format[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set prompt - OpenOrca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = []\n",
    "    mapper = {\"system\" : \"SYSTEM:\", \"human\" : \"USER:\", \"gpt\" : \"ASSISTANT:\"}\n",
    "    end_mapper = {\"system\" : \"\\n\\n\", \"human\" : \"\\n\", \"gpt\" : \"</s>\\n\"}\n",
    "    for convo in convos:\n",
    "        text = \"\".join(f\"{mapper[(turn := x['from'])]} {x['value']}{end_mapper[turn]}\" for x in convo)\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "train_dataset_in_prompt_format = train_dataset.map(formatting_prompts_func, batched = True,)\n",
    "print(train_dataset_in_prompt_format[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do model patching and add fast LoRA weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.2 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = r.value,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = lora_alpha.value,\n",
    "    lora_dropout = lora_dropout.value, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = True,\n",
    "    random_state = random_state.value,\n",
    "    max_seq_length = max_seq_length.value,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct the model trainer\n",
    "- Will train the model with TRL (Transformer Reinforcement Learning), with the SFT (Supervised Fine Tuning) trainer\n",
    "- Use the text column of the dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dec6dcc570a04c4f8f8e4c81b2483dc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/355500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_dir = \"./\" + new_model_id.value + \"/output\"\n",
    "logging_dir =  \"./\" + new_model_id.value + \"/logs\"\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset_in_prompt_format,\n",
    "    #eval_dataset = test_dataset_in_prompt_format,\n",
    "    max_seq_length = max_seq_length.value,\n",
    "    dataset_num_proc = dataset_num_proc.value,\n",
    "    dataset_text_field = \"text\",\n",
    "    args = TrainingArguments(\n",
    "        num_train_epochs = num_epochs.value,\n",
    "        per_device_train_batch_size = per_device_train_batch_size.value,\n",
    "        gradient_accumulation_steps = gradient_accumulation_steps.value,\n",
    "        warmup_steps = warmup_steps.value,\n",
    "        max_steps = max_steps.value,\n",
    "        learning_rate = learning_rate.value,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = logging_steps.value,\n",
    "        logging_dir = logging_dir,   \n",
    "        optim = optim.value,\n",
    "        weight_decay = weight_decay.value,\n",
    "        lr_scheduler_type = lr_scheduler_type.value,\n",
    "        seed = random_state.value,\n",
    "        #do_eval=True,\n",
    "        #evaluation_strategy=\"steps\",\n",
    "        #eval_steps = eval_steps.value,  \n",
    "        save_steps = save_steps.value,                \n",
    "        report_to = \"wandb\", \n",
    "        run_name = f\"{wandb_project}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\",\n",
    "        output_dir = output_dir,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show current memory stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 3090. Max memory = 23.691 GB.\n",
      "6.898 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/samssd/developments/genai-playground/language/05-fine-tuning/auto-sloth/wandb/run-20240216_145751-ybg43qd6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/corticalstackteam/ft-mistral-7b-metamathqa-sft/runs/ybg43qd6' target=\"_blank\">ft-mistral-7b-metamathqa-sft-2024-02-16-14-55</a></strong> to <a href='https://wandb.ai/corticalstackteam/ft-mistral-7b-metamathqa-sft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/corticalstackteam/ft-mistral-7b-metamathqa-sft' target=\"_blank\">https://wandb.ai/corticalstackteam/ft-mistral-7b-metamathqa-sft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/corticalstackteam/ft-mistral-7b-metamathqa-sft/runs/ybg43qd6' target=\"_blank\">https://wandb.ai/corticalstackteam/ft-mistral-7b-metamathqa-sft/runs/ybg43qd6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 03:54, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.076700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.151200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.905500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.719600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.598600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.665900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.640400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.546800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.539900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.497400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.511300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.451100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.443700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.502600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.435700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.461900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.533400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.471700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.459400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.481700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.490100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.542200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.479700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.479500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.455300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.459300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.440500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.497500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.445000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.483300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show memory stats after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244.0593 seconds used for training.\n",
      "4.07 minutes used for training.\n",
      "Peak reserved memory = 17.688 GB.\n",
      "Peak reserved memory for training = 10.79 GB.\n",
      "Peak reserved memory % of max memory = 74.661 %.\n",
      "Peak reserved memory for training % of max memory = 45.545 %.\n"
     ]
    }
   ],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get last checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./mistral-7b-metamathqa-sft/output/checkpoint-30'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "last_checkpoint = get_last_checkpoint(output_dir)\n",
    "last_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create HF model card"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the jinja template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_text = \"\"\"\n",
    "---\n",
    "license: {{ license }}\n",
    "---\n",
    "\n",
    "# {{ new_model_id }}\n",
    "\n",
    "{{ new_model_id }} is an SFT fine-tuned version of {{ model_id }} using the {{ train_dataset_name }} dataset.\n",
    "\n",
    "## Fine-tuning configuration\n",
    "### LoRA\n",
    "- r: {{ r }}\n",
    "- LoRA alpha: {{ lora_alpha }}\n",
    "- LoRA dropout: {{ lora_dropout }}\n",
    "\n",
    "### Training arguments\n",
    "- Epochs: {{ num_epochs }}\n",
    "- Batch size: {{ per_device_train_batch_size }}\n",
    "- Gradient accumulation steps: {{ gradient_accumulation_steps }}\n",
    "- Optimizer: {{ optim }}\n",
    "- Max steps: {{ max_steps }}\n",
    "- Learning_rate: {{ learning_rate }}\n",
    "- Weight decay: {{ weight_decay }}\n",
    "- Learning rate scheduler type: {{ lr_scheduler_type }}\n",
    "- Max seq length: {{ max_seq_length }}\n",
    "- 4-bit bnb: {{ four_bit_bnb }}\n",
    "\n",
    "Trained with [Unsloth](https://github.com/unslothai/unsloth) and Huggingface's TRL library.\n",
    "\n",
    "[<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png\" width=\"200\"/>](https://github.com/unslothai/unsloth)\n",
    "\"\"\"\n",
    "\n",
    "    # Create a Jinja template object\n",
    "jinja_template = Template(template_text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the template\n",
    "content = jinja_template.render(\n",
    "          license = license.value,\n",
    "          new_model_id = new_model_id.value,\n",
    "          model_id = model_id.value,\n",
    "          train_dataset_name = train_dataset_name.value,\n",
    "          r = r.value,\n",
    "          lora_alpha = lora_alpha.value,\n",
    "          lora_dropout = lora_dropout.value,\n",
    "          num_epochs = num_epochs.value,\n",
    "          per_device_train_batch_size = per_device_train_batch_size.value,\n",
    "          gradient_accumulation_steps = gradient_accumulation_steps.value,\n",
    "          optim = optim.value,\n",
    "          max_steps = max_steps.value,\n",
    "          learning_rate = learning_rate.value,\n",
    "          weight_decay = weight_decay.value,\n",
    "          lr_scheduler_type = lr_scheduler_type.value,\n",
    "          max_seq_length = max_seq_length.value,\n",
    "          four_bit_bnb = four_bit_bnb.value\n",
    "          )\n",
    "\n",
    "# Save the model card\n",
    "card = ModelCard(content)\n",
    "card.save(f\"{new_model_id.value}/README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 77.53 out of 106.1 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 20/32 [00:00<00:00, 43.24it/s]We will save to Disk and not RAM now.\n",
      "100%|██████████| 32/32 [00:04<00:00,  7.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "if False: model.push_to_hub_merged(f\"{username.value}/{new_model_id.value}\", tokenizer, save_method = \"merged_16bit\", token = HF_TOKEN)\n",
    "if True: model.save_pretrained_merged(f\"{new_model_id.value}\", tokenizer, save_method = \"merged_16bit\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/CorticalStack/mistral-7b-metamathqa-sft', endpoint='https://huggingface.co', repo_type='model', repo_id='CorticalStack/mistral-7b-metamathqa-sft')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api = HfApi(token=HF_TOKEN)\n",
    "api.create_repo(\n",
    "    repo_id=f\"{username.value}/{new_model_id.value}\",\n",
    "    repo_type=\"model\",\n",
    "    exist_ok=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f45dcf7d12774a10b4eb1b81b199d29a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7df409d43c4a4916a2ae2b28b1591d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70551baded574799aeeba215f3c5bc36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d315e29c9c462aa0051058ac57eba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98f75150b3ee4a44970cfb68019c4119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/CorticalStack/mistral-7b-metamathqa-sft/commit/c16588b550a4238a113c1b56f6e7e2825491236d', commit_message='Upload folder using huggingface_hub', commit_description='', oid='c16588b550a4238a113c1b56f6e7e2825491236d', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.upload_folder(\n",
    "    repo_id=f\"{username.value}/{new_model_id.value}\",\n",
    "    folder_path=new_model_id.value,\n",
    "    token=HF_TOKEN\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genaiplayground_tune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
