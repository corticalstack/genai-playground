{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import Layout\n",
    "import os\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "from jinja2 import Template\n",
    "from huggingface_hub import ModelCard, HfApi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set variables like HuggingFace token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()  # take environment variables from .env.\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huggingface Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e88bc073c8d422a85f98740556f115b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='CorticalStack', description='HF hub user', style=TextStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4ac6c2eaa1c49a085841dce6676675a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='mistral-7b-alpaca-sft', description='New model id', style=TextStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "833a6774d2684a0f9139a663f6379de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='apache-2.0', description='License', style=TextStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "username = widgets.Text(\n",
    "    value=\"CorticalStack\",\n",
    "    description='HF hub user',\n",
    "    disabled=False\n",
    ")\n",
    "username.style.description_width = 'initial'\n",
    "display(username)\n",
    "\n",
    "new_model_id = widgets.Text(\n",
    "    value=\"mistral-7b-alpaca-sft\",\n",
    "    description='New model id',\n",
    "    disabled=False\n",
    ")\n",
    "new_model_id.style.description_width = 'initial'\n",
    "display(new_model_id)\n",
    "\n",
    "license = widgets.Text(\n",
    "    value=\"apache-2.0\",\n",
    "    description='License',\n",
    "    disabled=False\n",
    ")\n",
    "license.style.description_width = 'initial'\n",
    "display(license)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f28857b97bd64c96853beebb21f89cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=256, description='r', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "725f24ed2ad94228bd6cf12a39cd2522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=128, description='lora alpha', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a66c557c804850a5833adeb379e81a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='lora dropout', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff9588e8a18c466b942f01d8114d8ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=3407, description='random state', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r = widgets.IntText(\n",
    "    value=256,\n",
    "    description='r',\n",
    "    disabled=False\n",
    ")\n",
    "r.style.description_width = 'initial'\n",
    "display(r)\n",
    "\n",
    "lora_alpha = widgets.IntText(\n",
    "    value=128,\n",
    "    description='lora alpha',\n",
    "    disabled=False\n",
    ")\n",
    "lora_alpha.style.description_width = 'initial'\n",
    "display(lora_alpha)\n",
    "\n",
    "lora_dropout = widgets.FloatText(\n",
    "    value=0,\n",
    "    description='lora dropout',\n",
    "    disabled=False\n",
    ")\n",
    "lora_dropout.style.description_width = 'initial'\n",
    "display(lora_dropout)\n",
    "\n",
    "random_state = widgets.IntText(\n",
    "    value=3407,\n",
    "    description='random state',\n",
    "    disabled=False\n",
    ")\n",
    "random_state.style.description_width = 'initial'\n",
    "display(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39534e2c76d344d185c0518e66500a37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='unsloth/mistral-7b-bnb-4bit', description='Model ID', style=TextStyle(description_width='initial')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96225520349842568da3a67a56aa068f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='yahma/alpaca-cleaned', description='Train dataset', style=TextStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "371944240e7b43cea149021710416398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=1, description='Number epochs', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f328ca0a6e3343068af7e3cc11ddb726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=4, description='Per device train batch size', style=DescriptionStyle(description_width='initial'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b213827a11cb48f4a819f9e7c131e02f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=4, description='Gradient accumulation steps', style=DescriptionStyle(description_width='initial'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca2f4dd299e14a2e8616c1a2dd4b7be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=4, description='Dataset num proc', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bf5cf122fdd477c9f4b95f2db3d7121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=1, description='Logging steps', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af8e5ebe87a146099dfca6e8492bd6a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='adamw_torch_fused', description='Optimizer', style=TextStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9a86c7e1e7a4df1b59b3afafd21fef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=5, description='Warmup steps', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32bc0ff3957946f69768fc218ea00ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=30, description='Max steps', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15b9928980bb415099f9ce859cc34226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=5, description='Eval steps', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3199f3b724f444fbb31e99cd1192cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=5, description='Save steps', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47fec1f0b45b480ea8bfe129e803d525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0002, description='Learning rate', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a7a07fce1dc48e8a58dad0a413d1d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.01, description='Weight decay', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "484e3bf5956a46eea88aa6b249596fe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='linear', description='LR schedule type', style=TextStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b268b28aaf4b9c8607363f6b33cacf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=2048, description='Max seq length', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "839f24b3a1d3450eabd90abc3f912879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='Four bit bnb', indent=False, style=CheckboxStyle(description_width='initial'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = widgets.Text(\n",
    "    value=\"unsloth/mistral-7b-bnb-4bit\",\n",
    "    description='Model ID',\n",
    "    disabled=False\n",
    ")\n",
    "model_id.style.description_width = 'initial'\n",
    "display(model_id)\n",
    "\n",
    "train_dataset_name = widgets.Text(\n",
    "    value=\"yahma/alpaca-cleaned\",\n",
    "    description='Train dataset',\n",
    "    disabled=False\n",
    ")\n",
    "train_dataset_name.style.description_width = 'initial'\n",
    "display(train_dataset_name)\n",
    "\n",
    "num_epochs = widgets.IntText(\n",
    "    value=1,\n",
    "    description='Number epochs',\n",
    "    disabled=False\n",
    ")\n",
    "num_epochs.style.description_width = 'initial'\n",
    "display(num_epochs)\n",
    "\n",
    "per_device_train_batch_size = widgets.IntText(\n",
    "    value=4,\n",
    "    description='Per device train batch size',\n",
    "    disabled=False\n",
    ")\n",
    "per_device_train_batch_size.style.description_width = 'initial'\n",
    "display(per_device_train_batch_size)\n",
    "\n",
    "gradient_accumulation_steps = widgets.IntText(\n",
    "    value=4,\n",
    "    description='Gradient accumulation steps',\n",
    "    disabled=False\n",
    ")\n",
    "gradient_accumulation_steps.style.description_width = 'initial'\n",
    "display(gradient_accumulation_steps)\n",
    "\n",
    "dataset_num_proc = widgets.IntText(\n",
    "    value=4,\n",
    "    description='Dataset num proc',\n",
    "    disabled=False\n",
    ")\n",
    "dataset_num_proc.style.description_width = 'initial'\n",
    "display(dataset_num_proc)\n",
    "\n",
    "logging_steps = widgets.IntText(\n",
    "    value=1,\n",
    "    description='Logging steps',\n",
    "    disabled=False\n",
    ")\n",
    "logging_steps.style.description_width = 'initial'\n",
    "display(logging_steps)\n",
    "\n",
    "optim = widgets.Text(\n",
    "    value=\"adamw_torch_fused\",\n",
    "    description='Optimizer',\n",
    "    disabled=False\n",
    ")\n",
    "optim.style.description_width = 'initial'\n",
    "display(optim)\n",
    "\n",
    "warmup_steps = widgets.IntText(\n",
    "    value=5,\n",
    "    description='Warmup steps',\n",
    "    disabled=False\n",
    ")\n",
    "warmup_steps.style.description_width = 'initial'\n",
    "display(warmup_steps)\n",
    "\n",
    "max_steps = widgets.IntText(\n",
    "    value=30,\n",
    "    description='Max steps',\n",
    "    disabled=False\n",
    ")\n",
    "max_steps.style.description_width = 'initial'\n",
    "display(max_steps)\n",
    "\n",
    "eval_steps = widgets.IntText(\n",
    "    value=5,\n",
    "    description='Eval steps',\n",
    "    disabled=False\n",
    ")\n",
    "eval_steps.style.description_width = 'initial'\n",
    "display(eval_steps)\n",
    "\n",
    "save_steps = widgets.IntText(\n",
    "    value=5,\n",
    "    description='Save steps',\n",
    "    disabled=False\n",
    ")\n",
    "save_steps.style.description_width = 'initial'\n",
    "display(save_steps)\n",
    "\n",
    "learning_rate = widgets.FloatText(\n",
    "    value=\"2e-4\",\n",
    "    description='Learning rate',\n",
    "    disabled=False\n",
    ")\n",
    "learning_rate.style.description_width = 'initial'\n",
    "display(learning_rate)\n",
    "\n",
    "weight_decay = widgets.FloatText(\n",
    "    value=\"0.01\",\n",
    "    description='Weight decay',\n",
    "    disabled=False\n",
    ")\n",
    "weight_decay.style.description_width = 'initial'\n",
    "display(weight_decay)\n",
    "\n",
    "lr_scheduler_type = widgets.Text(\n",
    "    value=\"linear\",\n",
    "    description='LR schedule type',\n",
    "    disabled=False\n",
    ")\n",
    "lr_scheduler_type.style.description_width = 'initial'\n",
    "display(lr_scheduler_type)\n",
    "\n",
    "max_seq_length = widgets.IntText(\n",
    "    value=2048,  #1024,\n",
    "    description='Max seq length',\n",
    "    disabled=False\n",
    ")\n",
    "max_seq_length.style.description_width = 'initial'\n",
    "display(max_seq_length)\n",
    "\n",
    "four_bit_bnb = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Four bit bnb',\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "four_bit_bnb.style.description_width = 'initial'\n",
    "display(four_bit_bnb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking training with weights & biases (wandb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcorticalstack\u001b[0m (\u001b[33mcorticalstackteam\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "wandb.login()\n",
    "\n",
    "wandb_project = \"ft-\" + new_model_id.value\n",
    "if len(wandb_project) > 0:\n",
    "    os.environ[\"WANDB_PROJECT\"] = wandb_project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset from the HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(train_dataset_name.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset records: 46584\n",
      "test dataset records: 5176\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Write a script for a commercial that is intended to show the effectiveness of a new wellness product.',\n",
       " 'output': \"(Cut to a person sitting in their office, rubbing their temples, looking stressed)\\n\\nNarrator: Are you feeling overwhelmed by everyday stressors?\\n\\n(Person sighs and leans back in their chair)\\n\\nNarrator: Introducing our new Wellness Product!\\n\\n(Cut to the product, displayed in all its glory)\\n\\nNarrator: With its calming blend of natural extracts and essential oils, this product is designed to help you feel calm, centered, and refreshed.\\n\\n(Cut to the person applying the product to their temples and taking a deep breath, closing their eyes)\\n\\nNarrator: Just a few moments with our Wellness Product can help reduce tension, improve focus, and increase overall well-being.\\n\\n(Cut to the person smiling, looking more relaxed)\\n\\nNarrator: Don't let stress weigh you down. Try our new Wellness Product and start feeling better today.\",\n",
       " 'input': 'New Wellness Product'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "train_test_split = dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "\n",
    "# Extract the training and testing datasets\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_dataset = train_test_split[\"test\"]\n",
    "\n",
    "print(f\"train dataset records: {len(train_dataset)}\")\n",
    "print(f\"test dataset records: {len(test_dataset)}\")\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Mistral patching release 2024.2\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.691 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.1.2.post301. CUDA = 8.6. CUDA Toolkit = 11.8.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.21. FA = True.\n",
      " \"-____-\"     Apache 2 free license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed `quantization_config` to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` attribute will be overwritten with the one you passed to `from_pretrained`.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_id.value,\n",
    "    max_seq_length = max_seq_length.value,\n",
    "    dtype = None, # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "    load_in_4bit = four_bit_bnb.value, # Use 4bit quantization to reduce memory usage. Can be False\n",
    "    token=HF_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def format_prompts(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9dcdce51afe409d9c4c7945e971101c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/46584 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d188336a984a518eba09ce40d8b4dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5176 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Write a script for a commercial that is intended to show the effectiveness of a new wellness product.\n",
      "\n",
      "### Input:\n",
      "New Wellness Product\n",
      "\n",
      "### Response:\n",
      "(Cut to a person sitting in their office, rubbing their temples, looking stressed)\n",
      "\n",
      "Narrator: Are you feeling overwhelmed by everyday stressors?\n",
      "\n",
      "(Person sighs and leans back in their chair)\n",
      "\n",
      "Narrator: Introducing our new Wellness Product!\n",
      "\n",
      "(Cut to the product, displayed in all its glory)\n",
      "\n",
      "Narrator: With its calming blend of natural extracts and essential oils, this product is designed to help you feel calm, centered, and refreshed.\n",
      "\n",
      "(Cut to the person applying the product to their temples and taking a deep breath, closing their eyes)\n",
      "\n",
      "Narrator: Just a few moments with our Wellness Product can help reduce tension, improve focus, and increase overall well-being.\n",
      "\n",
      "(Cut to the person smiling, looking more relaxed)\n",
      "\n",
      "Narrator: Don't let stress weigh you down. Try our new Wellness Product and start feeling better today.</s>\n"
     ]
    }
   ],
   "source": [
    "train_dataset_in_prompt_format = train_dataset.map(format_prompts, batched = True,)\n",
    "test_dataset_in_prompt_format = test_dataset.map(format_prompts, batched = True,)\n",
    "print(train_dataset_in_prompt_format[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do model patching and add fast LoRA weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.2 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = r.value,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = lora_alpha.value,\n",
    "    lora_dropout = lora_dropout.value, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = True,\n",
    "    random_state = random_state.value,\n",
    "    max_seq_length = max_seq_length.value,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct the model trainer\n",
    "- Will train the model with TRL (Transformer Reinforcement Learning), with the SFT (Supervised Fine Tuning) trainer\n",
    "- Use the text column of the dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69cd00111c80463bbc37c60e00173f50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/46584 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d549318931c14d298461cb4ade84ede2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/5176 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_dir = \"./\" + new_model_id.value + \"/output\"\n",
    "logging_dir =  \"./\" + new_model_id.value + \"/logs\"\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset_in_prompt_format,\n",
    "    eval_dataset = test_dataset_in_prompt_format,\n",
    "    max_seq_length = max_seq_length.value,\n",
    "    dataset_num_proc = dataset_num_proc.value,\n",
    "    dataset_text_field = \"text\",\n",
    "    args = TrainingArguments(\n",
    "        num_train_epochs = num_epochs.value,\n",
    "        per_device_train_batch_size = per_device_train_batch_size.value,\n",
    "        gradient_accumulation_steps = gradient_accumulation_steps.value,\n",
    "        warmup_steps = warmup_steps.value,\n",
    "        max_steps = max_steps.value,\n",
    "        learning_rate = learning_rate.value,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = logging_steps.value,\n",
    "        logging_dir = logging_dir,   \n",
    "        optim = optim.value,\n",
    "        weight_decay = weight_decay.value,\n",
    "        lr_scheduler_type = lr_scheduler_type.value,\n",
    "        seed = random_state.value,\n",
    "        do_eval=True,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps = eval_steps.value,  \n",
    "        save_steps = save_steps.value,                \n",
    "        report_to = \"wandb\", \n",
    "        run_name = f\"{wandb_project}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\",\n",
    "        output_dir = output_dir,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show current memory stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 3090. Max memory = 23.691 GB.\n",
      "6.898 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/samssd/developments/genai-playground/language/05-fine-tuning/auto-sloth/wandb/run-20240215_153455-6j1jtavd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/corticalstackteam/ft-mistral-7b-alpaca-sft/runs/6j1jtavd' target=\"_blank\">ft-mistral-7b-alpaca-sft-2024-02-15-15-34</a></strong> to <a href='https://wandb.ai/corticalstackteam/ft-mistral-7b-alpaca-sft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/corticalstackteam/ft-mistral-7b-alpaca-sft' target=\"_blank\">https://wandb.ai/corticalstackteam/ft-mistral-7b-alpaca-sft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/corticalstackteam/ft-mistral-7b-alpaca-sft/runs/6j1jtavd' target=\"_blank\">https://wandb.ai/corticalstackteam/ft-mistral-7b-alpaca-sft/runs/6j1jtavd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 1:11:33, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.861100</td>\n",
       "      <td>0.894354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.789900</td>\n",
       "      <td>0.833206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.856500</td>\n",
       "      <td>0.816988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.789400</td>\n",
       "      <td>0.808283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.791900</td>\n",
       "      <td>0.803483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.797900</td>\n",
       "      <td>0.799118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show memory stats after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4301.8237 seconds used for training.\n",
      "71.7 minutes used for training.\n",
      "Peak reserved memory = 22.771 GB.\n",
      "Peak reserved memory for training = 15.873 GB.\n",
      "Peak reserved memory % of max memory = 96.117 %.\n",
      "Peak reserved memory for training % of max memory = 67.0 %.\n"
     ]
    }
   ],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get last checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./mistral-7b-alpaca-sft/output/checkpoint-30'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "last_checkpoint = get_last_checkpoint(output_dir)\n",
    "last_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create HF model card"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the jinja template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_text = \"\"\"\n",
    "---\n",
    "license: {{ license }}\n",
    "---\n",
    "\n",
    "# {{ new_model_id }}\n",
    "\n",
    "{{ new_model_id }} is an SFT fine-tuned version of {{ model_id }} using the {{ train_dataset_name }} dataset.\n",
    "\n",
    "## Fine-tuning configuration\n",
    "### LoRA\n",
    "- r: {{ r }}\n",
    "- LoRA alpha: {{ lora_alpha }}\n",
    "- LoRA dropout: {{ lora_dropout }}\n",
    "\n",
    "### Training arguments\n",
    "- Epochs: {{ num_epochs }}\n",
    "- Batch size: {{ per_device_train_batch_size }}\n",
    "- Gradient accumulation steps: {{ gradient_accumulation_steps }}\n",
    "- Optimizer: {{ optim }}\n",
    "- Max steps: {{ max_steps }}\n",
    "- Learning_rate: {{ learning_rate }}\n",
    "- Weight decay: {{ weight_decay }}\n",
    "- Learning rate scheduler type: {{ lr_scheduler_type }}\n",
    "- Max seq length: {{ max_seq_length }}\n",
    "- 4-bit bnb: {{ four_bit_bnb }}\n",
    "\n",
    "Trained with [Unsloth](https://github.com/unslothai/unsloth) and Huggingface's TRL library.\n",
    "\n",
    "[<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png\" width=\"200\"/>](https://github.com/unslothai/unsloth)\n",
    "\"\"\"\n",
    "\n",
    "    # Create a Jinja template object\n",
    "jinja_template = Template(template_text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the template\n",
    "content = jinja_template.render(\n",
    "          license = license.value,\n",
    "          new_model_id = new_model_id.value,\n",
    "          model_id = model_id.value,\n",
    "          train_dataset_name = train_dataset_name.value,\n",
    "          r = r.value,\n",
    "          lora_alpha = lora_alpha.value,\n",
    "          lora_dropout = lora_dropout.value,\n",
    "          num_epochs = num_epochs.value,\n",
    "          per_device_train_batch_size = per_device_train_batch_size.value,\n",
    "          gradient_accumulation_steps = gradient_accumulation_steps.value,\n",
    "          optim = optim.value,\n",
    "          max_steps = max_steps.value,\n",
    "          learning_rate = learning_rate.value,\n",
    "          weight_decay = weight_decay.value,\n",
    "          lr_scheduler_type = lr_scheduler_type.value,\n",
    "          max_seq_length = max_seq_length.value,\n",
    "          four_bit_bnb = four_bit_bnb.value\n",
    "          )\n",
    "\n",
    "# Save the model card\n",
    "card = ModelCard(content)\n",
    "card.save(f\"{new_model_id.value}/README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 79.62 out of 106.1 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:25<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "if False: model.push_to_hub_merged(f\"{username.value}/{new_model_id.value}\", tokenizer, save_method = \"merged_16bit\", token = HF_TOKEN)\n",
    "if True: model.save_pretrained_merged(f\"{new_model_id.value}\", tokenizer, save_method = \"merged_16bit\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f007d708b47a42a99262612a9cffffe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbde9a630a194d928f173cd94a07cf0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a74e697e9464008b2eb67698d78ac9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "898c8101172c407b83816ae10702ef3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb8224958fbd4d1cbb00216a269cec0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/CorticalStack/mistral-7b-alpaca-sft/commit/a23b6bcdff99735543644928f7fa085a8bab51bb', commit_message='Upload folder using huggingface_hub', commit_description='', oid='a23b6bcdff99735543644928f7fa085a8bab51bb', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api = HfApi(token=HF_TOKEN)\n",
    "api.create_repo(\n",
    "    repo_id=f\"{username.value}/{new_model_id.value}\",\n",
    "    repo_type=\"model\",\n",
    "    exist_ok=True,\n",
    ")\n",
    "api.upload_folder(\n",
    "    repo_id=f\"{username.value}/{new_model_id.value}\",\n",
    "    folder_path=new_model_id.value,\n",
    "    token=HF_TOKEN\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genaiplayground_tune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
