{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from huggingface_hub import HfApi, create_repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set variables like HuggingFace token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()  # take environment variables from .env.\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify HuggingFace Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "845f303c340b4177b4bae4514c386fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='CorticalStack/mistral-7b-alpaca-sft', description='Model ID', style=TextStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af6396659953451e83418ed825be835b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='CorticalStack/mistral-7b-alpaca-awq', description='New model ID', style=TextStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = widgets.Text(\n",
    "    value='CorticalStack/mistral-7b-alpaca-sft',\n",
    "    description='Model ID',\n",
    "    disabled=False\n",
    ")\n",
    "model_id.style.description_width = 'initial'\n",
    "display(model_id)\n",
    "\n",
    "new_model_id = widgets.Text(\n",
    "    value='CorticalStack/mistral-7b-alpaca-awq',\n",
    "    description='New model ID',\n",
    "    disabled=False\n",
    ")\n",
    "new_model_id.style.description_width = 'initial'\n",
    "display(new_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWQ quant parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ebc321e96648ceac365dd1dc873637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=128, description='Q group size', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bf187164f71419b8fcae619980ff14a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=4, description='W bit', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b48e1c1102fa449bb5eec4111e583c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='GEMM', description='Version', style=TextStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "433ed82be8c548ae94b1b4d0d930b98d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='Zero point', indent=False, style=CheckboxStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q_group_size = widgets.IntText(\n",
    "    value=128,\n",
    "    description='Q group size',\n",
    "    disabled=False\n",
    ")\n",
    "q_group_size.style.description_width = 'initial'\n",
    "display(q_group_size)\n",
    "\n",
    "w_bit = widgets.IntText(\n",
    "    value=4,\n",
    "    description='W bit',\n",
    "    disabled=False\n",
    ")\n",
    "w_bit.style.description_width = 'initial'\n",
    "display(w_bit)\n",
    "\n",
    "version = widgets.Text(\n",
    "    value=\"GEMM\",\n",
    "    description='Version',\n",
    "    disabled=False\n",
    ")\n",
    "version.style.description_width = 'initial'\n",
    "display(version)\n",
    "\n",
    "zero_point = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Zero point',\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "zero_point.style.description_width = 'initial'\n",
    "display(zero_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the source model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = str(model_id.value).split('/')\n",
    "if not os.path.isdir(model_name[1]):\n",
    "    !git clone https://huggingface.co/{model_id.value}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the AWQ quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b66620768ace48d2b5c902c1aca32585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.conda/envs/genaiplayground_quant/lib/python3.11/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (77697 > 8192). Running this sequence through the model will result in indexing errors\n",
      "AWQ: 100%|██████████| 32/32 [14:01<00:00, 26.31s/it]\n",
      "WARNING:root:`quant_config.json` is being deprecated in the future in favor of quantization_config in config.json.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('pastiche-crown-clown-7b-dare-awq/tokenizer_config.json',\n",
       " 'pastiche-crown-clown-7b-dare-awq/special_tokens_map.json',\n",
       " 'pastiche-crown-clown-7b-dare-awq/tokenizer.model',\n",
       " 'pastiche-crown-clown-7b-dare-awq/added_tokens.json',\n",
       " 'pastiche-crown-clown-7b-dare-awq/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAFETENSORS = True # @param {text:\"boolean\"}\n",
    "\n",
    "# Install AutoAWQ\n",
    "# !git clone https://github.com/casper-hansen/AutoAWQ\n",
    "# %cd AutoAWQ\n",
    "# !pip install -e .\n",
    "# !pip install git+https://github.com/huggingface/transformers\n",
    "# !pip install zstandard\n",
    "\n",
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "quant_config = { \"zero_point\": zero_point.value, \"q_group_size\": q_group_size.value, \"w_bit\": w_bit.value, \"version\": version.value }\n",
    "\n",
    "# Load model\n",
    "PATH = f\"{model_name[1]}\"\n",
    "model = AutoAWQForCausalLM.from_pretrained(PATH, safetensors=SAFETENSORS)\n",
    "tokenizer = AutoTokenizer.from_pretrained(PATH, trust_remote_code=True)\n",
    "\n",
    "# Quantize\n",
    "model.quantize(tokenizer, quant_config=quant_config)\n",
    "\n",
    "# Save quantized model\n",
    "new_model_name = str(new_model_id.value).split('/')\n",
    "model.save_quantized(new_model_name[1])\n",
    "tokenizer.save_pretrained(new_model_name[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create HF model card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import ModelCard\n",
    "from jinja2 import Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0a7c2bdd14c4fd2931c7f69809d84d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='apache-2.0', description='License', style=TextStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "license = widgets.Text(\n",
    "    value=\"apache-2.0\",\n",
    "    description='License',\n",
    "    disabled=False\n",
    ")\n",
    "license.style.description_width = 'initial'\n",
    "display(license)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the jinja template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_text = \"\"\"\n",
    "---\n",
    "license: {{ license }}\n",
    "---\n",
    "\n",
    "# {{ new_model_id }}\n",
    "\n",
    "{{ new_model_id }} is an AWQ quantised version of [{{ model_id }}]({{ model_url }}).\n",
    "\n",
    "### About AWQ\n",
    "AWQ is an efficient, accurate and blazing-fast low-bit weight quantization method, currently supporting 4-bit quantization. Compared to GPTQ, it offers faster Transformers-based inference with equivalent or better quality compared to the most commonly used GPTQ settings.\n",
    "\n",
    "AWQ models are currently supported on Linux and Windows, with NVidia GPUs only. macOS users: please use GGUF models instead.\n",
    "\n",
    "It is supported by:\n",
    "\n",
    "- [Text Generation Webui](https://github.com/oobabooga/text-generation-webui) - using Loader: AutoAWQ\n",
    "- [vLLM](https://github.com/vllm-project/vllm) - version 0.2.2 or later for support for all model types.\n",
    "- [Hugging Face Text Generation Inference (TGI)](https://github.com/huggingface/text-generation-inference)\n",
    "- [Transformers](https://huggingface.co/docs/transformers) version 4.35.0 and later, from any code or client that supports Transformers\n",
    "- [AutoAWQ](https://github.com/casper-hansen/AutoAWQ) - for use from Python code\n",
    "\n",
    "### AWQ configuration\n",
    "- Zero point: {{ zero_point }}\n",
    "- Q group size: {{q_group_size}}\n",
    "- W bit: {{ w_bit}}\n",
    "- Version: {{ version }}\n",
    "\"\"\"\n",
    "\n",
    "    # Create a Jinja template object\n",
    "jinja_template = Template(template_text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the template\n",
    "content = jinja_template.render(\n",
    "          license = license.value,\n",
    "          new_model_id = new_model_id.value,\n",
    "          model_id = model_id.value,\n",
    "          model_url = \"https://huggingface.co/\" + model_id.value,\n",
    "          zero_point = zero_point.value,\n",
    "          q_group_size = q_group_size.value,\n",
    "          w_bit = w_bit.value,\n",
    "          version = version.value\n",
    "          )\n",
    "\n",
    "# Save the model card\n",
    "card = ModelCard(content)\n",
    "card.save(f\"{new_model_name[1]}/README.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push to HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5fe85ea8362443892612f3a37b94b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "480803752d5f4ebe9b102a3bb01397ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07713e7c604b4050ab882b6b7cdeca6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/CorticalStack/pastiche-crown-clown-7b-dare-awq/commit/4c0b22b9d69909b7806985ad40a5a7c3b046b877', commit_message='Upload folder using huggingface_hub', commit_description='', oid='4c0b22b9d69909b7806985ad40a5a7c3b046b877', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api = HfApi()\n",
    "\n",
    "api.create_repo(new_model_id.value, exist_ok=True, repo_type=\"model\", token=HF_TOKEN)\n",
    "api.upload_folder(\n",
    "    folder_path=new_model_name[1],\n",
    "    repo_id=new_model_id.value,\n",
    "    token=HF_TOKEN\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genaiplayground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
